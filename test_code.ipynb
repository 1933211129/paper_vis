{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4dd9f06",
   "metadata": {},
   "source": [
    "# 测试并发"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6eadfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "开始测试 10 个并发请求\n",
      "============================================================\n",
      "\n",
      "[13:35:17.543] 请求 #1 开始\n",
      "[13:35:17.543] 请求 #2 开始\n",
      "[13:35:17.543] 请求 #3 开始\n",
      "[13:35:17.543] 请求 #4 开始\n",
      "[13:35:17.543] 请求 #5 开始\n",
      "[13:35:17.544] 请求 #6 开始\n",
      "[13:35:17.544] 请求 #7 开始\n",
      "[13:35:17.544] 请求 #8 开始\n",
      "[13:35:17.544] 请求 #9 开始\n",
      "[13:35:17.544] 请求 #10 开始\n",
      "[13:35:19.495] 请求 #6 成功 (耗时: 1.95秒)\n",
      "[13:35:19.496] 请求 #4 成功 (耗时: 1.95秒)\n",
      "[13:35:19.500] 请求 #9 成功 (耗时: 1.96秒)\n",
      "[13:35:19.502] 请求 #2 成功 (耗时: 1.96秒)\n",
      "[13:35:19.504] 请求 #3 成功 (耗时: 1.96秒)\n",
      "[13:35:19.509] 请求 #7 成功 (耗时: 1.97秒)\n",
      "[13:35:19.509] 请求 #5 成功 (耗时: 1.97秒)\n",
      "[13:35:19.568] 请求 #10 成功 (耗时: 2.02秒)\n",
      "[13:35:19.568] 请求 #8 成功 (耗时: 2.02秒)\n",
      "[13:35:19.869] 请求 #1 成功 (耗时: 2.33秒)\n",
      "\n",
      "============================================================\n",
      "测试完成！总耗时: 2.33秒\n",
      "============================================================\n",
      "\n",
      "成功: 10/10\n",
      "失败: 0/10\n",
      "平均响应时间: 2.01秒\n",
      "\n",
      "如果总耗时接近平均响应时间，说明支持真正的并发\n",
      "如果总耗时接近平均响应时间×请求数，说明是串行执行\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# DeepSeek API配置\n",
    "API_KEY = \"sk-8de35978ccec41e39e2b9ebfc90b7aa1\"\n",
    "API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "MODEL = \"deepseek-chat\"\n",
    "\n",
    "async def call_deepseek_api(session, request_id):\n",
    "    \"\"\"异步调用DeepSeek API\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": f\"这是测试请求 #{request_id}，请简单回复收到。\"}\n",
    "        ],\n",
    "        \"max_tokens\": 50\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] 请求 #{request_id} 开始\")\n",
    "    \n",
    "    try:\n",
    "        async with session.post(API_URL, json=payload, headers=headers) as response:\n",
    "            result = await response.json()\n",
    "            end_time = time.time()\n",
    "            elapsed = end_time - start_time\n",
    "            \n",
    "            if response.status == 200:\n",
    "                print(f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] 请求 #{request_id} 成功 (耗时: {elapsed:.2f}秒)\")\n",
    "                return {\"id\": request_id, \"status\": \"success\", \"time\": elapsed, \"response\": result}\n",
    "            else:\n",
    "                print(f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] 请求 #{request_id} 失败: {response.status}\")\n",
    "                return {\"id\": request_id, \"status\": \"failed\", \"time\": elapsed, \"error\": result}\n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] 请求 #{request_id} 异常: {str(e)}\")\n",
    "        return {\"id\": request_id, \"status\": \"error\", \"time\": elapsed, \"error\": str(e)}\n",
    "\n",
    "async def test_concurrent_requests(num_requests=5):\n",
    "    \"\"\"测试并发请求\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"开始测试 {num_requests} 个并发请求\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # 创建5个并发任务\n",
    "        tasks = [call_deepseek_api(session, i+1) for i in range(num_requests)]\n",
    "        # 同时执行所有任务\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"测试完成！总耗时: {total_time:.2f}秒\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # 统计结果\n",
    "    success_count = sum(1 for r in results if r[\"status\"] == \"success\")\n",
    "    failed_count = sum(1 for r in results if r[\"status\"] != \"success\")\n",
    "    avg_time = sum(r[\"time\"] for r in results) / len(results)\n",
    "    \n",
    "    print(f\"成功: {success_count}/{num_requests}\")\n",
    "    print(f\"失败: {failed_count}/{num_requests}\")\n",
    "    print(f\"平均响应时间: {avg_time:.2f}秒\")\n",
    "    print(f\"\\n如果总耗时接近平均响应时间，说明支持真正的并发\")\n",
    "    print(f\"如果总耗时接近平均响应时间×请求数，说明是串行执行\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 运行测试\n",
    "results = await test_concurrent_requests(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a03c4",
   "metadata": {},
   "source": [
    "# 测试规则抽取摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "587d48e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试修复后的abstract提取函数:\n",
      "============================================================\n",
      "测试文件1: /Users/xiaokong/task/2025/paper_vis/vis/md/af705d1369467b0aa55cb59354a84a0e.md\n",
      "找到abstract标题在第14行: # ---- A B S T R A C T  ----\n",
      "True\n",
      "✓ 成功提取Abstract内容:\n",
      "Keywords: Brain-inspired continual learning Context similarity assessment Neuronal discriminative expansion Neuronal selective reuse Sparse spiking neural networks  \n",
      "\n",
      "Biological brains have the capability to adaptively coordinate relevant neuronal populations based on the task context to learn continuously changing tasks in real-world environments. However, existing spiking neural network-based continual learning algorithms treat each task equally, ignoring the guiding role of different task similarity associations for network learning, which limits knowledge utilization efficiency. Inspired by the context-dependent plasticity mechanism of the brain, we propose a Similarity-based Context Aware Spiking Neural Network (SCA-SNN) continual learning algorithm to efficiently accomplish task incremental learning and class incremental learning. Based on contextual similarity across tasks, the SCA-SNN model can adaptively reuse neurons from previous tasks that are beneficial for new tasks (the more similar, the more neurons are reused) and flexibly expand new neurons for the new task (the more similar, the fewer neurons are expanded). Selective reuse and discriminative expansion significantly improve the utilization of previous knowledge and reduce energy consumption. Extensive experimental results on CIFAR100, ImageNet generalized datasets, and FMNIST-MNIST, SVHN-CIFAR100 mixed datasets show that our SCA-SNN model achieves superior performance compared to both SNN-based and DNN-based continual learning algorithms. Additionally, our algorithm has the capability to adaptively select similar groups of neurons for related tasks, offering a promising approach to enhancing the biological interpretability of efficient continual learning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_abstract_from_md(file_path):\n",
    "    \"\"\"\n",
    "    从Markdown文件中提取Abstract部分的正文内容\n",
    "    \n",
    "    参数:\n",
    "        file_path: str 或 Path对象,指向.md文件的路径\n",
    "    \n",
    "    返回:\n",
    "        tuple: (bool, str) 第一个元素表示是否找到摘要,第二个元素是摘要文本\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 读取文件内容\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # 按行分割\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        # 更灵活的abstract匹配模式\n",
    "        def is_abstract_header(line):\n",
    "            \"\"\"判断一行是否是abstract标题\"\"\"\n",
    "            # 去除首尾空白\n",
    "            stripped = line.strip()\n",
    "            \n",
    "            # 必须以#开头\n",
    "            if not stripped.startswith('#'):\n",
    "                return False\n",
    "            \n",
    "            # 去掉#号和后面的空白字符\n",
    "            after_hash = stripped[1:].strip()\n",
    "            \n",
    "            # 如果#后面没有内容,不是有效标题\n",
    "            if not after_hash:\n",
    "                return False\n",
    "            \n",
    "            # 去掉所有空格、标点符号和特殊字符,只保留字母\n",
    "            # 这样可以匹配 \"A B S T R A C T\", \"Abstract:\", \"Abstract.\" 等各种形式\n",
    "            cleaned = re.sub(r'[^a-zA-Z\\u4e00-\\u9fff]', '', after_hash)\n",
    "            \n",
    "            # 转为小写进行比较\n",
    "            normalized = cleaned.lower()\n",
    "            \n",
    "            # 检查是否匹配abstract的各种形式\n",
    "            abstract_variants = [\n",
    "                'abstract',\n",
    "                'abstracts',\n",
    "                '摘要',\n",
    "            ]\n",
    "            \n",
    "            # 直接检查是否匹配\n",
    "            if normalized in abstract_variants:\n",
    "                return True\n",
    "            \n",
    "            return False\n",
    "        \n",
    "        # 查找abstract开始位置\n",
    "        abstract_start = -1\n",
    "        for i, line in enumerate(lines):\n",
    "            if is_abstract_header(line):\n",
    "                abstract_start = i\n",
    "                print(f\"找到abstract标题在第{i+1}行: {line.strip()}\")\n",
    "                break\n",
    "        \n",
    "        # 如果没找到abstract标题,返回(False, \"\")\n",
    "        if abstract_start == -1:\n",
    "            print(\"未找到abstract标题\")\n",
    "            return (False, \"\")\n",
    "        \n",
    "        # 查找下一个一级标题(abstract结束位置)\n",
    "        abstract_end = len(lines)\n",
    "        for i in range(abstract_start + 1, len(lines)):\n",
    "            # 检查是否是一级标题(以单个#开头,后面跟空格和非空白字符)\n",
    "            stripped = lines[i].strip()\n",
    "            if stripped.startswith('#') and not stripped.startswith('##'):\n",
    "                # 确保#后面有内容\n",
    "                after_hash = stripped[1:].strip()\n",
    "                if after_hash:\n",
    "                    abstract_end = i\n",
    "                    break\n",
    "        \n",
    "        # 提取abstract内容(跳过标题行本身)\n",
    "        abstract_lines = lines[abstract_start + 1:abstract_end]\n",
    "        \n",
    "        # 去除首尾空行\n",
    "        while abstract_lines and not abstract_lines[0].strip():\n",
    "            abstract_lines.pop(0)\n",
    "        while abstract_lines and not abstract_lines[-1].strip():\n",
    "            abstract_lines.pop()\n",
    "        \n",
    "        # 合并为文本\n",
    "        abstract_text = '\\n'.join(abstract_lines).strip()\n",
    "        \n",
    "        # 检查摘要文本的单词数是否大于30\n",
    "        word_count = len(abstract_text.split())\n",
    "        if word_count <= 30:\n",
    "            print(f\"摘要文本单词数不足: {word_count} 个单词 (需要大于30个)\")\n",
    "            return (False, \"\")\n",
    "        \n",
    "        return (True, abstract_text)\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"错误:文件未找到 - {file_path}\")\n",
    "        return (False, \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"错误:处理文件时出现异常 - {str(e)}\")\n",
    "        return (False, \"\")\n",
    "\n",
    "\n",
    "# 测试函数\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"测试修复后的abstract提取函数:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 测试第一个文件\n",
    "    test_file1 = \"/Users/xiaokong/task/2025/paper_vis/vis/md/af705d1369467b0aa55cb59354a84a0e.md\"\n",
    "    print(f\"测试文件1: {test_file1}\")\n",
    "    found, abstract1 = extract_abstract_from_md(test_file1)\n",
    "    if found:\n",
    "        print(found)\n",
    "        print(\"✓ 成功提取Abstract内容:\")\n",
    "        print(abstract1)\n",
    "    else:\n",
    "        print(found)\n",
    "        print(\"✗ 未找到Abstract内容\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba024aa",
   "metadata": {},
   "source": [
    "# LLM抽取摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62b04926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qy/w56f01vd5h35lc_7plwj_k_r0000gn/T/ipykernel_1880/74677222.py:43: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  @validator('background_problem', 'method_approach', 'result', 'conclusion_contribution')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 开始测试摘要语步分析系统\n",
      "================================================================================\n",
      "\n",
      "📁 测试文件 1: af705d1369467b0aa55cb59354a84a0e.md\n",
      "\n",
      "================================================================================\n",
      "🔍 开始分析文件: /Users/xiaokong/task/2025/paper_vis/vis/md/af705d1369467b0aa55cb59354a84a0e.md\n",
      "================================================================================\n",
      "找到abstract标题在第14行: # ---- A B S T R A C T  ----\n",
      "✅ 规则提取成功，使用摘要文本 (长度: 1752 字符)\n",
      "\n",
      "🤖 开始LLM语步分析...\n",
      "🔄 LLM调用尝试 1/3\n",
      "📝 LLM原始回复: {\n",
      "    \"Background/Problem\": \"Biological brains adaptively learn tasks, but current spiking neural networks ignore task similarity, limiting knowledge reuse efficiency in continual learning.\",\n",
      "    \"Met...\n",
      "✅ JSON解析成功\n",
      "✅ Pydantic验证成功\n",
      "\n",
      "✅ 分析完成！耗时: 7.19秒\n",
      "📊 语步分析结果:\n",
      "   • 背景/问题: Biological brains adaptively learn tasks, but current spiking neural networks ignore task similarity, limiting knowledge reuse efficiency in continual learning.\n",
      "   • 方法/途径: We propose SCA-SNN, which adaptively reuses or expands neurons based on contextual task similarity to improve knowledge utilization and reduce energy.\n",
      "   • 结果: Experiments on multiple datasets show SCA-SNN outperforms existing SNN and DNN continual learning methods in task and class incremental learning.\n",
      "   • 结论/贡献: Our algorithm enhances biological interpretability and offers an efficient, adaptive approach to continual learning with selective neuron reuse and expansion.\n",
      "\n",
      "⏳ 等待2秒后处理下一个文件...\n",
      "\n",
      "📁 测试文件 2: 3791465d4e18e4033b5c7bd322c44df2.md\n",
      "\n",
      "================================================================================\n",
      "🔍 开始分析文件: /Users/xiaokong/task/2025/paper_vis/vis/md/3791465d4e18e4033b5c7bd322c44df2.md\n",
      "================================================================================\n",
      "未找到abstract标题\n",
      "⚠️ 规则提取失败，使用前5000字符\n",
      "📄 使用前5000字符 (实际长度: 5000 字符)\n",
      "\n",
      "🤖 开始LLM语步分析...\n",
      "🔄 LLM调用尝试 1/3\n",
      "📝 LLM原始回复: {\n",
      "    \"Background/Problem\": \"Cognitive decline in Alzheimer's disease is highly variable, and current biomarkers like amyloid-beta and tau explain only 20-40% of cognitive impairment variance.\",\n",
      "    \"...\n",
      "✅ JSON解析成功\n",
      "✅ Pydantic验证成功\n",
      "\n",
      "✅ 分析完成！耗时: 7.22秒\n",
      "📊 语步分析结果:\n",
      "   • 背景/问题: Cognitive decline in Alzheimer's disease is highly variable, and current biomarkers like amyloid-beta and tau explain only 20-40% of cognitive impairment variance.\n",
      "   • 方法/途径: We performed CSF proteomics on 3,397 individuals from six cohorts and used machine learning to derive the CSF YWHAG:NPTX2 synapse protein ratio.\n",
      "   • 结果: CSF YWHAG:NPTX2 explained significant additional variance in cognitive impairment and predicted conversion to mild cognitive impairment and dementia over 15 years.\n",
      "   • 结论/贡献: CSF YWHAG:NPTX2 is a robust prognostic biomarker for cognitive resilience versus Alzheimer's onset and progression, highlighting synapse dysfunction as a core driver.\n",
      "\n",
      "================================================================================\n",
      "📈 测试结果汇总\n",
      "================================================================================\n",
      "✅ 成功: 2/2\n",
      "❌ 失败: 0/2\n"
     ]
    }
   ],
   "source": [
    "# LLM摘要语步提取系统\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import time\n",
    "from typing import Optional, Dict, Any\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from pathlib import Path\n",
    "\n",
    "# DeepSeek API配置\n",
    "API_KEY = \"sk-8de35978ccec41e39e2b9ebfc90b7aa1\"\n",
    "API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "MODEL = \"deepseek-chat\"\n",
    "\n",
    "# Pydantic Schema定义\n",
    "class AbstractSteps(BaseModel):\n",
    "    \"\"\"学术摘要四步语步结构\"\"\"\n",
    "    background_problem: str = Field(\n",
    "        ..., \n",
    "        alias=\"Background/Problem\",\n",
    "        description=\"背景/问题描述，不超过35个英文单词\",\n",
    "        max_length=200\n",
    "    )\n",
    "    method_approach: str = Field(\n",
    "        ..., \n",
    "        alias=\"Method/Approach\", \n",
    "        description=\"方法/途径，不超过35个英文单词\",\n",
    "        max_length=200\n",
    "    )\n",
    "    result: str = Field(\n",
    "        ..., \n",
    "        alias=\"Result\",\n",
    "        description=\"结果，不超过35个英文单词\", \n",
    "        max_length=200\n",
    "    )\n",
    "    conclusion_contribution: str = Field(\n",
    "        ..., \n",
    "        alias=\"Conclusion/Contribution\",\n",
    "        description=\"结论/贡献，不超过35个英文单词\",\n",
    "        max_length=200\n",
    "    )\n",
    "    \n",
    "    @validator('background_problem', 'method_approach', 'result', 'conclusion_contribution')\n",
    "    def validate_word_count(cls, v):\n",
    "        \"\"\"验证每个字段不超过35个英文单词\"\"\"\n",
    "        word_count = len(v.split())\n",
    "        if word_count > 35:\n",
    "            raise ValueError(f\"字段超过35个单词限制: {word_count} 个单词\")\n",
    "        return v\n",
    "    \n",
    "    class Config:\n",
    "        allow_population_by_field_name = True\n",
    "        json_encoders = {\n",
    "            str: lambda v: v.strip()\n",
    "        }\n",
    "\n",
    "# 系统提示词\n",
    "SYSTEM_PROMPT = \"\"\"You are a highly specialized expert in academic text analysis and structured data extraction. Your **sole task** is to accurately identify and extract the four standard rhetorical steps of the academic abstract from the provided text, and output the result in a **strictly valid JSON format**.\n",
    "\n",
    "**[Core Task & Output]**\n",
    "1. The input text might be a **clean abstract** OR a **document segment** (including metadata, title, authors, and the abstract).\n",
    "2. If the text is a segment, you **must first locate the Abstract** based on its semantic features (a condensed summary of background, method, and results), and **ignore all surrounding noise**.\n",
    "3. Decompose the identified Abstract content into the following four standard rhetorical steps: Background/Problem, Method/Approach, Result, Conclusion/Contribution.\n",
    "\n",
    "**[Format Requirements]**\n",
    "1. **MUST** output a JSON object that strictly conforms to the provided schema.\n",
    "2. **ABSOLUTELY DO NOT** output any introductory text, explanations, or text outside the raw JSON object.\n",
    "3. Each summary **MUST NOT exceed 35 English words**, focusing on high-level summarization.\"\"\"\n",
    "\n",
    "# 用户提示词模板\n",
    "USER_PROMPT_TEMPLATE = \"\"\"Please analyze the text provided below. And decompose its content into the four standard rhetorical steps.\n",
    "\n",
    "**[Text to Analyze]**\n",
    "{text}\n",
    "\n",
    "**[Expected JSON Schema]**\n",
    "Please adhere strictly to this JSON structure for your output:\n",
    "\n",
    "{{\n",
    "    \"Background/Problem\": \"The concise English summary for this step, no more than 35 words.\",\n",
    "    \"Method/Approach\": \"The concise English summary for this step, no more than 35 words.\",\n",
    "    \"Result\": \"The concise English summary for this step, no more than 35 words.\",\n",
    "    \"Conclusion/Contribution\": \"The concise English summary for this step, no more than 35 words.\"\n",
    "}}\"\"\"\n",
    "\n",
    "async def call_deepseek_api(text: str, max_retries: int = 2) -> Optional[AbstractSteps]:\n",
    "    \"\"\"\n",
    "    调用DeepSeek API进行摘要语步提取\n",
    "    \n",
    "    Args:\n",
    "        text: 要分析的文本\n",
    "        max_retries: 最大重试次数\n",
    "    \n",
    "    Returns:\n",
    "        AbstractSteps对象或None\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    user_prompt = USER_PROMPT_TEMPLATE.format(text=text)\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.1  # 降低随机性，提高一致性\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            print(f\"🔄 LLM调用尝试 {attempt + 1}/{max_retries + 1}\")\n",
    "            \n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.post(API_URL, json=payload, headers=headers) as response:\n",
    "                    if response.status != 200:\n",
    "                        print(f\"❌ API调用失败: HTTP {response.status}\")\n",
    "                        if attempt < max_retries:\n",
    "                            await asyncio.sleep(1)  # 等待1秒后重试\n",
    "                            continue\n",
    "                        return None\n",
    "                    \n",
    "                    result = await response.json()\n",
    "                    \n",
    "                    # 提取回复内容\n",
    "                    if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "                        content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                        print(f\"📝 LLM原始回复: {content[:200]}...\")\n",
    "                        \n",
    "                        # 尝试解析JSON\n",
    "                        try:\n",
    "                            # 清理可能的markdown代码块标记\n",
    "                            if content.startswith(\"```json\"):\n",
    "                                content = content[7:]\n",
    "                            if content.endswith(\"```\"):\n",
    "                                content = content[:-3]\n",
    "                            content = content.strip()\n",
    "                            \n",
    "                            # 解析JSON\n",
    "                            json_data = json.loads(content)\n",
    "                            print(f\"✅ JSON解析成功\")\n",
    "                            \n",
    "                            # 验证并创建Pydantic对象\n",
    "                            abstract_steps = AbstractSteps(**json_data)\n",
    "                            print(f\"✅ Pydantic验证成功\")\n",
    "                            return abstract_steps\n",
    "                            \n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"❌ JSON解析失败: {str(e)}\")\n",
    "                            if attempt < max_retries:\n",
    "                                print(f\"🔄 准备重试...\")\n",
    "                                await asyncio.sleep(1)\n",
    "                                continue\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"❌ Pydantic验证失败: {str(e)}\")\n",
    "                            if attempt < max_retries:\n",
    "                                print(f\"🔄 准备重试...\")\n",
    "                                await asyncio.sleep(1)\n",
    "                                continue\n",
    "                    else:\n",
    "                        print(f\"❌ API响应格式异常\")\n",
    "                        if attempt < max_retries:\n",
    "                            await asyncio.sleep(1)\n",
    "                            continue\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ 网络请求异常: {str(e)}\")\n",
    "            if attempt < max_retries:\n",
    "                await asyncio.sleep(1)\n",
    "                continue\n",
    "    \n",
    "    print(f\"❌ 所有重试尝试均失败\")\n",
    "    return None\n",
    "\n",
    "def extract_text_for_llm(file_path: str) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    从markdown文件中提取用于LLM分析的文本\n",
    "    \n",
    "    Args:\n",
    "        file_path: markdown文件路径\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (是否找到摘要, 分析文本)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 首先尝试规则提取摘要\n",
    "        found, abstract_text = extract_abstract_from_md(file_path)\n",
    "        \n",
    "        if found and abstract_text.strip():\n",
    "            print(f\"✅ 规则提取成功，使用摘要文本 (长度: {len(abstract_text)} 字符)\")\n",
    "            return True, abstract_text\n",
    "        else:\n",
    "            # 规则提取失败，读取前5000字符\n",
    "            print(f\"⚠️ 规则提取失败，使用前5000字符\")\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # 取前5000字符\n",
    "            text_for_llm = content[:5000]\n",
    "            print(f\"📄 使用前5000字符 (实际长度: {len(text_for_llm)} 字符)\")\n",
    "            return False, text_for_llm\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 文件读取异常: {str(e)}\")\n",
    "        return False, \"\"\n",
    "\n",
    "async def analyze_abstract_steps(file_path: str) -> Optional[AbstractSteps]:\n",
    "    \"\"\"\n",
    "    分析markdown文件的摘要语步\n",
    "    \n",
    "    Args:\n",
    "        file_path: markdown文件路径\n",
    "    \n",
    "    Returns:\n",
    "        AbstractSteps对象或None\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🔍 开始分析文件: {file_path}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 1. 提取分析文本\n",
    "    is_abstract, analysis_text = extract_text_for_llm(file_path)\n",
    "    \n",
    "    if not analysis_text.strip():\n",
    "        print(f\"❌ 无法获取分析文本\")\n",
    "        return None\n",
    "    \n",
    "    # 2. 调用LLM分析\n",
    "    print(f\"\\n🤖 开始LLM语步分析...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    result = await call_deepseek_api(analysis_text)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    \n",
    "    if result:\n",
    "        print(f\"\\n✅ 分析完成！耗时: {elapsed:.2f}秒\")\n",
    "        print(f\"📊 语步分析结果:\")\n",
    "        print(f\"   • 背景/问题: {result.background_problem}\")\n",
    "        print(f\"   • 方法/途径: {result.method_approach}\")\n",
    "        print(f\"   • 结果: {result.result}\")\n",
    "        print(f\"   • 结论/贡献: {result.conclusion_contribution}\")\n",
    "        return result\n",
    "    else:\n",
    "        print(f\"\\n❌ 分析失败！耗时: {elapsed:.2f}秒\")\n",
    "        return None\n",
    "\n",
    "# 测试函数\n",
    "async def test_abstract_analysis():\n",
    "    \"\"\"测试摘要分析功能\"\"\"\n",
    "    print(f\"\\n🧪 开始测试摘要语步分析系统\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 测试文件列表\n",
    "    test_files = [\n",
    "        \"/Users/xiaokong/task/2025/paper_vis/vis/md/af705d1369467b0aa55cb59354a84a0e.md\",\n",
    "        \"/Users/xiaokong/task/2025/paper_vis/vis/md/3791465d4e18e4033b5c7bd322c44df2.md\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, file_path in enumerate(test_files, 1):\n",
    "        print(f\"\\n📁 测试文件 {i}: {Path(file_path).name}\")\n",
    "        \n",
    "        if not Path(file_path).exists():\n",
    "            print(f\"❌ 文件不存在: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        result = await analyze_abstract_steps(file_path)\n",
    "        results.append((file_path, result))\n",
    "        \n",
    "        if i < len(test_files):\n",
    "            print(f\"\\n⏳ 等待2秒后处理下一个文件...\")\n",
    "            await asyncio.sleep(2)\n",
    "    \n",
    "    # 汇总结果\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"📈 测试结果汇总\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    success_count = sum(1 for _, result in results if result is not None)\n",
    "    total_count = len(results)\n",
    "    \n",
    "    print(f\"✅ 成功: {success_count}/{total_count}\")\n",
    "    print(f\"❌ 失败: {total_count - success_count}/{total_count}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 运行测试\n",
    "if __name__ == \"__main__\":\n",
    "    # 运行异步测试\n",
    "    results = await test_abstract_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379489f4",
   "metadata": {},
   "source": [
    "# 标题规范化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19611b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1级标题列表: ['# Abstract', '# 1. Introduction', '# 2. Methodology', '# 3. Results', '# Conclusion', '# References']\n",
      "标题结构分析: {'total_headings': 12, 'h1_count': 12, 'h2_count': 0, 'h3_count': 0, 'h4_count': 0, 'h5_count': 0, 'h6_count': 0, 'headings_by_level': {1: ['# Abstract', '# 1. Introduction', '# 1.1 Background', '# 1.1.1 Historical Context', '# 1.2 Related Work', '# 1.2.1 Previous Studies', '# 2. Methodology', '# 2.1 Data Collection', '# 2.1.1 Sampling Strategy', '# 3. Results', '# Conclusion', '# References']}}\n",
      "文件 /Users/xiaokong/task/2025/paper_vis/vis/md/3791465d4e18e4033b5c7bd322c44df2.md 的1级标题: ['# A cerebrospinal fluid synaptic protein biomarker for prediction of cognitive resilience versus decline in Alzheimer’s disease', '# A list of authors and their affiliations appears at the end of the paper', '# Check for updates', '# Multicohort CSF proteomics for AD biomarker discovery', '# CSF YWHAG:NPTX2 versus established neurodegeneration AD biomarkers', '# CSF YWHAG:NPTX2 in normal aging and ADAD', '# CSF YWHAG:NPTX2 associations with future AD progression', '# Five defined CSF YWHAG:NPTX2 groups for cognitive prognosis', '# Partial plasma proteomic surrogate of CSF YWHAG:NPTX2', '# Discussion', '# Online content', '# References', '# Article', '# Methods', '# Participants', '# Proteomics', '# CI stage classification', '# $\\\\mathbf{A}\\\\mathbf{+}\\\\mathbf{T_{1}}\\\\mathbf{+}$ versus A– $\\\\cdot\\\\mathbf{T}_{1}$ − classification', '# Statistical analyses', '# Reporting summary', '# Data availability', '# Code availability', '# References', '# Acknowledgements', '# Author contributions', '# Competing interests', '# standardized effect size', '# nature portfolio', '# Statistics', '# Software and code', '# Data', '#', '# Research involving human participants, their data, or biological material', '# PARTICIPANTS', '# BioFINDER2', '# Field-specific reporting', '# Life sciences study design', '# Reporting for specific materials, systems and methods', '# Materials & experimental systems Methods', '# Plants']\n"
     ]
    }
   ],
   "source": [
    "# 标题层级规范化系统\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "class HeadingNormalizer:\n",
    "    \"\"\"标题层级规范化器\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"初始化规范化器\"\"\"\n",
    "        # 定义已知的无编号一级标题关键词\n",
    "        self.unnumbered_h1_keywords = [\n",
    "            'Abstract', 'Introduction', 'Conclusion', 'References', 'Appendix',\n",
    "            'Acknowledgments', 'Acknowledgements', 'Bibliography', 'Index',\n",
    "            'Preface', 'Foreword', 'Summary', 'Executive Summary',\n",
    "            'Table of Contents', 'List of Figures', 'List of Tables',\n",
    "            'Nomenclature', 'Glossary', 'Abbreviations'\n",
    "        ]\n",
    "        \n",
    "        # 编译正则表达式模式（按优先级排序）\n",
    "        self.patterns = [\n",
    "            # 三级标题: # 1.1.1. 标题\n",
    "            (r'^(\\s*)#\\s+(\\d+\\.\\d+\\.\\d+\\.\\s+.*)$', r'\\1### \\2'),\n",
    "            \n",
    "            # 二级标题: # 1.1. 标题  \n",
    "            (r'^(\\s*)#\\s+(\\d+\\.\\d+\\.\\s+.*)$', r'\\1## \\2'),\n",
    "            \n",
    "            # 一级标题: # 1. 标题\n",
    "            (r'^(\\s*)#\\s+(\\d+\\.\\s+.*)$', r'\\1# \\2'),\n",
    "            \n",
    "            # 无编号一级标题: # Abstract, # Conclusion 等\n",
    "            (r'^(\\s*)#\\s+(' + '|'.join(self.unnumbered_h1_keywords) + r')(\\s*.*)$', r'\\1# \\2\\3'),\n",
    "        ]\n",
    "    \n",
    "    def normalize_headings(self, markdown_text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        对学术论文的Markdown文本进行标题层级规范化，并返回所有1级标题列表\n",
    "        \n",
    "        将PDF解析工具错误标记的所有一级标题(#)根据编号模式恢复为正确的标题层级\n",
    "        \n",
    "        Args:\n",
    "            markdown_text: 包含论文内容的完整Markdown文本字符串\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: 所有1级标题列表\n",
    "        \"\"\"\n",
    "        \n",
    "        # 按行分割文本\n",
    "        lines = markdown_text.split('\\n')\n",
    "        normalized_lines = []\n",
    "        h1_headings = []  # 存储真正的1级标题\n",
    "        \n",
    "        # 统计信息\n",
    "        stats = {\n",
    "            'h1_numbered': 0,      # 编号一级标题\n",
    "            'h1_unnumbered': 0,    # 无编号一级标题  \n",
    "            'h2': 0,              # 二级标题\n",
    "            'h3': 0,              # 三级标题\n",
    "            'unchanged': 0        # 未匹配的标题\n",
    "        }\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            original_line = line\n",
    "            processed = False\n",
    "            \n",
    "            # 检查是否是以#开头的标题行\n",
    "            if re.match(r'^\\s*#\\s+', line):\n",
    "                # 按优先级尝试匹配各种模式\n",
    "                for pattern, replacement in self.patterns:\n",
    "                    if re.match(pattern, line):\n",
    "                        # 执行替换\n",
    "                        new_line = re.sub(pattern, replacement, line)\n",
    "                        normalized_lines.append(new_line)\n",
    "                        \n",
    "                        # 统计处理结果并收集1级标题\n",
    "                        if '###' in new_line:\n",
    "                            stats['h3'] += 1\n",
    "                        elif '##' in new_line:\n",
    "                            stats['h2'] += 1\n",
    "                        elif '# ' in new_line and not re.match(r'^\\s*##', new_line):\n",
    "                            # 只有真正的1级标题才收集\n",
    "                            if re.match(r'^\\s*#\\s+\\d+\\.', new_line):\n",
    "                                # 编号的一级标题 (如 # 1. Introduction)\n",
    "                                stats['h1_numbered'] += 1\n",
    "                                h1_headings.append(new_line.strip())\n",
    "                            elif self._is_unnumbered_h1(new_line):\n",
    "                                # 无编号的一级标题 (如 # Abstract, # Conclusion)\n",
    "                                stats['h1_unnumbered'] += 1\n",
    "                                h1_headings.append(new_line.strip())\n",
    "                        \n",
    "                        processed = True\n",
    "                        break\n",
    "                \n",
    "                # 如果没有匹配任何模式，检查是否为无编号的1级标题\n",
    "                if not processed:\n",
    "                    normalized_lines.append(line)\n",
    "                    if self._is_unnumbered_h1(line):\n",
    "                        stats['h1_unnumbered'] += 1\n",
    "                        h1_headings.append(line.strip())\n",
    "                    else:\n",
    "                        stats['unchanged'] += 1\n",
    "            else:\n",
    "                # 非标题行，直接添加\n",
    "                normalized_lines.append(line)\n",
    "        \n",
    "        return h1_headings\n",
    "    \n",
    "    def _is_unnumbered_h1(self, line: str) -> bool:\n",
    "        \"\"\"\n",
    "        判断是否为1级标题\n",
    "        \n",
    "        规则：\n",
    "        1. 所有以#开头的都是标题\n",
    "        2. 通过序号格式判断级别：\n",
    "           - 1, 1. → 1级标题\n",
    "           - 1.1, 1.1. → 2级标题\n",
    "           - 1.1.1, 1.1.1. → 3级标题\n",
    "        3. 没有序号的标题默认为1级标题\n",
    "        \n",
    "        Args:\n",
    "            line: 标题行\n",
    "        \n",
    "        Returns:\n",
    "            bool: 是否为1级标题\n",
    "        \"\"\"\n",
    "        # 提取标题文本（去掉#和空格）\n",
    "        title_text = re.sub(r'^\\s*#+\\s*', '', line).strip()\n",
    "        \n",
    "        # 1. 检查是否匹配已知的无编号1级标题关键词（精确匹配）\n",
    "        for keyword in self.unnumbered_h1_keywords:\n",
    "            if title_text.lower() == keyword.lower():\n",
    "                return True\n",
    "        \n",
    "        # 2. 检查序号格式来判断标题级别\n",
    "        # 2.1 匹配1级标题格式：纯数字开头（如 \"1\", \"1.\", \"2\", \"2.\"）\n",
    "        if re.match(r'^\\d+\\.?\\s+', title_text):\n",
    "            # 检查是否包含小数点（如 1.1, 1.1.1）\n",
    "            if not re.search(r'\\d+\\.\\d+', title_text):\n",
    "                return True  # 这是1级标题\n",
    "        \n",
    "        # 2.2 检查是否为2级或3级标题格式\n",
    "        if re.search(r'\\d+\\.\\d+', title_text):\n",
    "            return False  # 这是2级或3级标题\n",
    "        \n",
    "        # 3. 没有序号的标题默认为1级标题\n",
    "        return True\n",
    "\n",
    "\n",
    "    def extract_headings_only(self, markdown_text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        提取规范化后的所有标题行\n",
    "        \n",
    "        Args:\n",
    "            markdown_text: Markdown文本\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: 所有标题行的列表\n",
    "        \"\"\"\n",
    "        lines = markdown_text.split('\\n')\n",
    "        headings = []\n",
    "        \n",
    "        for line in lines:\n",
    "            if re.match(r'^\\s*#+\\s+', line):\n",
    "                headings.append(line.strip())\n",
    "        \n",
    "        return headings\n",
    "    \n",
    "    def extract_h1_headings(self, markdown_text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        提取所有1级标题，按在文档中出现的顺序返回\n",
    "        \n",
    "        Args:\n",
    "            markdown_text: Markdown文本\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: 1级标题行的列表，每个元素是完整的原始行字符串（包含#号）\n",
    "        \"\"\"\n",
    "        lines = markdown_text.split('\\n')\n",
    "        h1_headings = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            # 检查是否为标题行\n",
    "            if line.startswith('#'):\n",
    "                # 检查是否为1级标题\n",
    "                if self._is_unnumbered_h1(line):\n",
    "                    h1_headings.append(line)  # 返回完整的原始行（包含#号）\n",
    "        \n",
    "        return h1_headings\n",
    "\n",
    "    def analyze_heading_structure(self, markdown_text: str) -> dict:\n",
    "        \"\"\"\n",
    "        分析标题结构，返回层级统计信息\n",
    "        \n",
    "        Args:\n",
    "            markdown_text: Markdown文本\n",
    "        \n",
    "        Returns:\n",
    "            dict: 标题结构分析结果\n",
    "        \"\"\"\n",
    "        headings = self.extract_headings_only(markdown_text)\n",
    "        \n",
    "        structure = {\n",
    "            'total_headings': len(headings),\n",
    "            'h1_count': 0,\n",
    "            'h2_count': 0, \n",
    "            'h3_count': 0,\n",
    "            'h4_count': 0,\n",
    "            'h5_count': 0,\n",
    "            'h6_count': 0,\n",
    "            'headings_by_level': {}\n",
    "        }\n",
    "        \n",
    "        for heading in headings:\n",
    "            # 计算标题层级\n",
    "            level = len(re.match(r'^#+', heading).group())\n",
    "            structure[f'h{level}_count'] += 1\n",
    "            \n",
    "            if level not in structure['headings_by_level']:\n",
    "                structure['headings_by_level'][level] = []\n",
    "            structure['headings_by_level'][level].append(heading)\n",
    "        \n",
    "        return structure\n",
    "\n",
    "    def process_markdown_file(self, file_path: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        处理实际的Markdown文件\n",
    "        \n",
    "        Args:\n",
    "            file_path: Markdown文件路径\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: 所有1级标题列表\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 读取文件\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # 规范化处理，返回1级标题列表\n",
    "            h1_headings = self.normalize_headings(content)\n",
    "            \n",
    "            return h1_headings\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            return []\n",
    "\n",
    "# 运行测试\n",
    "if __name__ == \"__main__\":\n",
    "    # 创建规范化器实例\n",
    "    normalizer = HeadingNormalizer()\n",
    "    \n",
    "    # 测试用例1: 模拟PDF解析后的错误标题格式\n",
    "    test_text_1 = \"\"\"# Abstract\n",
    "This is the abstract content.\n",
    "\n",
    "# 1. Introduction\n",
    "This is the introduction.\n",
    "\n",
    "# 1.1 Background\n",
    "Background information here.\n",
    "\n",
    "# 1.1.1 Historical Context\n",
    "Historical context details.\n",
    "\n",
    "# 1.2 Related Work\n",
    "Related work section.\n",
    "\n",
    "# 1.2.1 Previous Studies\n",
    "Previous studies details.\n",
    "\n",
    "# 2. Methodology\n",
    "Methodology section.\n",
    "\n",
    "# 2.1 Data Collection\n",
    "Data collection methods.\n",
    "\n",
    "# 2.1.1 Sampling Strategy\n",
    "Sampling strategy details.\n",
    "\n",
    "# 3. Results\n",
    "Results section.\n",
    "\n",
    "# Conclusion\n",
    "This is the conclusion.\n",
    "\n",
    "# References\n",
    "References list.\"\"\"\n",
    "    \n",
    "    # 规范化处理，返回1级标题列表\n",
    "    h1_headings = normalizer.normalize_headings(test_text_1)\n",
    "    print(\"1级标题列表:\", h1_headings)\n",
    "    \n",
    "    # 分析标题结构\n",
    "    structure = normalizer.analyze_heading_structure(test_text_1)\n",
    "    print(\"标题结构分析:\", structure)\n",
    "    \n",
    "    # 可选：处理实际文件\n",
    "    test_files = [\n",
    "         \"/Users/xiaokong/task/2025/paper_vis/vis/md/3791465d4e18e4033b5c7bd322c44df2.md\",\n",
    "    ]\n",
    "    \n",
    "    for file_path in test_files:\n",
    "        if Path(file_path).exists():\n",
    "            h1_headings = normalizer.process_markdown_file(file_path)\n",
    "            print(f\"文件 {file_path} 的1级标题:\", h1_headings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b340660a",
   "metadata": {},
   "source": [
    "# LLM标题映射"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a8a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "标题映射LLM模块\n",
    "利用LLM对论文章节标题进行分类映射到四个标准泳道\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "from typing import List, Dict, Optional, Any\n",
    "import time\n",
    "\n",
    "class TitleMappingLLM:\n",
    "    \"\"\"标题映射LLM处理器\"\"\"\n",
    "    \n",
    "    def __init__(self, api_url: str = None, api_key: str = None, model: str = None):\n",
    "        \"\"\"\n",
    "        初始化LLM处理器\n",
    "        \n",
    "        Args:\n",
    "            api_url: LLM API地址\n",
    "            api_key: API密钥\n",
    "            model: 使用的模型名称\n",
    "        \"\"\"\n",
    "        # 写死的LLM配置\n",
    "        self.api_url = api_url or \"https://api.deepseek.com/v1/chat/completions\"\n",
    "        self.api_key = api_key or \"sk-8de35978ccec41e39e2b9ebfc90b7aa1\"\n",
    "        self.model = model or \"deepseek-chat\"\n",
    "        \n",
    "        # 设置日志\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # 系统提示词\n",
    "        self.system_prompt = \"\"\"You are a top-tier **Academic Paper Structure Analyst** specializing in **cross-disciplinary semantic filtering and classification**. Your task is to accurately map chapter titles, which represent the **core research logic flow**, from a provided list of titles into four standard swimlanes.\n",
    "\n",
    "**[Core Filtering and Mapping Rules]**\n",
    "1.  **Filtering (Noise Reduction):** You must **ignore and discard** the following types of titles:\n",
    "    * **Non-chapter content:** Paper main titles, author lists, publication metadata (e.g., \"Article\", \"Online content\", \"Check for updates\", \"Reporting summary\", \"Data availability\", \"Author contributions\", \"Competing interests\", etc.).\n",
    "    * **Boundary anchors:** \"Abstract\" (or its variants), \"References\", \"Acknowledgements\", \"Appendix\".\n",
    "2.  **Lane Assignment (Classification):** Only assign the filtered **valid core chapters** to the following **Four Standard Swimlanes**.\n",
    "3.  **Quota Constraint (Max: 2):** The number of titles assigned to each standard swimlane **must not exceed two (Max: 2)**. If multiple titles belong to the same swimlane, you must select the core titles that best represent the function of that swimlane.\n",
    "\n",
    "**[Four Standard Swimlanes]**\n",
    "1.  Context & Related Work\n",
    "2.  Methodology & Setup\n",
    "3.  Results & Analysis\n",
    "4.  Conclusion\n",
    "\n",
    "**[CRITICAL FORMATTING REQUIREMENTS]**\n",
    "1.  **Strictly and uniquely** output a JSON object conforming to the JSON structure.\n",
    "2.  The Key must be the **Standard Swimlane Name**, and the Value must be an **array** containing the **EXACT original title strings**.\n",
    "3.  **PRESERVE EXACT FORMAT:** You MUST preserve the exact original format of titles including ALL symbols, numbers, punctuation, capitalization, and spacing (e.g., \"# 1. Introduction\", \"# 2. Related Work\", etc.).\n",
    "4.  **Absolutely forbid** outputting any explanations, preambles, summaries, or extra text.\"\"\"\n",
    "\n",
    "        # 用户提示词模板\n",
    "        self.user_prompt_template = \"\"\"Please analyze the **raw title list** provided below, which originates from a paper parser. Strictly adhere to the **filtering and quota constraints** rules specified in the system instructions to classify and map the core chapter titles into the four standard swimlanes.\n",
    "\n",
    "**[Title List to be Processed]**\n",
    "{title_list}\n",
    "\n",
    "**[Example of Desired JSON Structure]**\n",
    "Please strictly output your results according to the following concise structure, where the **Key is the Swimlane Name and the Value is an array of EXACT original title strings** (preserving all formatting including \"#\", numbers, punctuation):\n",
    "\n",
    "{{\n",
    "  \"Context & Related Work\": [\"# 1. Introduction\", \"# 2. Related Work\"],\n",
    "  \"Methodology & Setup\": [\"# 3. Methodology\"],\n",
    "  \"Results & Analysis\": [\"# 4. Results\", \"# 5. Discussion\"],\n",
    "  \"Conclusion\": [\"# 6. Conclusion\"]\n",
    "}}\"\"\"\n",
    "\n",
    "    def _call_llm_api(self, messages: List[Dict[str, str]], max_retries: int = 3) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        调用LLM API\n",
    "        \n",
    "        Args:\n",
    "            messages: 消息列表\n",
    "            max_retries: 最大重试次数\n",
    "            \n",
    "        Returns:\n",
    "            LLM响应内容\n",
    "        \"\"\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.1,  # 低温度确保一致性\n",
    "            \"max_tokens\": 1000\n",
    "        }\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    self.api_url,\n",
    "                    headers=headers,\n",
    "                    json=payload,\n",
    "                    timeout=30\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                else:\n",
    "                    self.logger.warning(f\"API调用失败，状态码: {response.status_code}, 响应: {response.text}\")\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                self.logger.warning(f\"API调用异常 (尝试 {attempt + 1}/{max_retries}): {e}\")\n",
    "                \n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)  # 指数退避\n",
    "                \n",
    "        self.logger.error(\"LLM API调用失败，已达到最大重试次数\")\n",
    "        return None\n",
    "\n",
    "    def _parse_json_response(self, response: str) -> Optional[Dict[str, List[str]]]:\n",
    "        \"\"\"\n",
    "        解析LLM返回的JSON响应\n",
    "        \n",
    "        Args:\n",
    "            response: LLM响应字符串\n",
    "            \n",
    "        Returns:\n",
    "            解析后的字典，如果解析失败返回None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 尝试直接解析\n",
    "            result = json.loads(response)\n",
    "            \n",
    "            # 验证结构\n",
    "            expected_keys = {\n",
    "                \"Context & Related Work\",\n",
    "                \"Methodology & Setup\", \n",
    "                \"Results & Analysis\",\n",
    "                \"Conclusion\"\n",
    "            }\n",
    "            \n",
    "            if not all(key in result for key in expected_keys):\n",
    "                self.logger.warning(\"JSON结构不完整，缺少必要的键\")\n",
    "                return None\n",
    "                \n",
    "            # 验证值类型\n",
    "            for key, value in result.items():\n",
    "                if not isinstance(value, list):\n",
    "                    self.logger.warning(f\"键 '{key}' 的值不是列表类型\")\n",
    "                    return None\n",
    "                    \n",
    "            return result\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            self.logger.warning(f\"JSON解析失败: {e}\")\n",
    "            \n",
    "            # 尝试提取JSON部分\n",
    "            try:\n",
    "                # 查找JSON开始和结束位置\n",
    "                start_idx = response.find('{')\n",
    "                end_idx = response.rfind('}')\n",
    "                \n",
    "                if start_idx != -1 and end_idx != -1 and end_idx > start_idx:\n",
    "                    json_str = response[start_idx:end_idx + 1]\n",
    "                    result = json.loads(json_str)\n",
    "                    return result\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "                \n",
    "            return None\n",
    "\n",
    "    def map_titles(self, title_list: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        将标题列表映射到四个标准泳道\n",
    "        \n",
    "        Args:\n",
    "            title_list: 原始标题列表\n",
    "            \n",
    "        Returns:\n",
    "            干净的映射结果字典\n",
    "        \"\"\"\n",
    "        if not title_list:\n",
    "            self.logger.error(\"标题列表为空\")\n",
    "            return {}\n",
    "            \n",
    "        if not self.api_key:\n",
    "            self.logger.error(\"API密钥未设置\")\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            # 构建用户提示词\n",
    "            title_list_str = \"\\n\".join([f\"'{title}'\" for title in title_list])\n",
    "            user_prompt = self.user_prompt_template.format(title_list=title_list_str)\n",
    "            \n",
    "            # 构建消息\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            \n",
    "            self.logger.info(f\"开始处理 {len(title_list)} 个标题\")\n",
    "            \n",
    "            # 调用LLM API\n",
    "            response = self._call_llm_api(messages)\n",
    "            \n",
    "            if response is None:\n",
    "                self.logger.error(\"LLM API调用失败\")\n",
    "                return {}\n",
    "            \n",
    "            # 解析响应\n",
    "            result = self._parse_json_response(response)\n",
    "            \n",
    "            if result is None:\n",
    "                self.logger.error(f\"LLM响应解析失败，原始响应: {response}\")\n",
    "                return {}\n",
    "            \n",
    "            # 验证结果\n",
    "            total_mapped = sum(len(titles) for titles in result.values())\n",
    "            self.logger.info(f\"成功映射 {total_mapped} 个标题到四个泳道\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"标题映射过程中发生异常: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def map_titles_with_debug(self, title_list: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        带调试信息的标题映射\n",
    "        \n",
    "        Args:\n",
    "            title_list: 原始标题列表\n",
    "            \n",
    "        Returns:\n",
    "            包含详细调试信息的映射结果\n",
    "        \"\"\"\n",
    "        debug_info = {\n",
    "            \"input_titles\": title_list,\n",
    "            \"input_count\": len(title_list),\n",
    "            \"api_url\": self.api_url,\n",
    "            \"model\": self.model,\n",
    "            \"timestamp\": time.time()\n",
    "        }\n",
    "        \n",
    "        result = self.map_titles(title_list)\n",
    "        result[\"debug_info\"] = debug_info\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"测试函数\"\"\"\n",
    "    # 示例使用\n",
    "    title_list = [\n",
    "        \"# A Q-learning approach to the continuous control problem of robot inverted pendulum balancing\",\n",
    "        \"# Corresponding Author:\",\n",
    "        \"# Abstract\",\n",
    "        \"# 1. Introduction\",\n",
    "        \"# 2. Proposed approach and background\", \n",
    "        \"# 3. Methodologies\",\n",
    "        \"# 4. Results and discussion\",\n",
    "        \"# 5. Conclusion\",\n",
    "        \"# Acknowledgements\",\n",
    "        \"# References\"\n",
    "    ]\n",
    "    \n",
    "    # 初始化处理器（需要设置API密钥）\n",
    "    mapper = TitleMappingLLM(\n",
    "        api_key=\"your-api-key-here\",  # 替换为实际的API密钥\n",
    "        model=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "    \n",
    "    # 执行映射\n",
    "    result = mapper.map_titles_with_debug(title_list)\n",
    "    \n",
    "    print(\"=== 标题映射结果 ===\")\n",
    "    print(f\"成功: {result['success']}\")\n",
    "    \n",
    "    if result['success']:\n",
    "        print(\"映射结果:\")\n",
    "        print(json.dumps(result['result'], indent=2, ensure_ascii=False))\n",
    "        print(\"\\n统计信息:\")\n",
    "        print(json.dumps(result['statistics'], indent=2, ensure_ascii=False))\n",
    "    else:\n",
    "        print(f\"错误: {result['error']}\")\n",
    "        if 'raw_response' in result:\n",
    "            print(f\"原始响应: {result['raw_response']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75dab40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f95e5e07",
   "metadata": {},
   "source": [
    "# 标题内容分割-抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e012416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "根据标题提取Markdown文件内容的工具\n",
    "\n",
    "功能：\n",
    "- 根据标题列表和指定标题，提取该标题到下一个标题之间的内容\n",
    "- 支持处理最后一个标题的情况（提取到文件结束）\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class ContentExtractor:\n",
    "    \"\"\"根据标题提取Markdown文件内容的类\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"初始化内容提取器\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _normalizeHeading(self, heading: str) -> str:\n",
    "        \"\"\"\n",
    "        标准化标题格式，用于灵活匹配\n",
    "        \n",
    "        Args:\n",
    "            heading: 原始标题\n",
    "        \n",
    "        Returns:\n",
    "            str: 标准化后的标题\n",
    "        \"\"\"\n",
    "        if not heading:\n",
    "            return \"\"\n",
    "        \n",
    "        # 去除首尾空格\n",
    "        normalized = heading.strip()\n",
    "        \n",
    "        # 如果标题以#开头，保留#和后面的内容\n",
    "        if normalized.startswith('#'):\n",
    "            # 提取#后面的内容\n",
    "            content = normalized[1:].strip()\n",
    "            return f\"#{content}\"\n",
    "        else:\n",
    "            # 如果没有#，直接返回去除空格后的内容\n",
    "            return normalized\n",
    "    \n",
    "    def _extractHeadingContent(self, heading: str) -> str:\n",
    "        \"\"\"\n",
    "        提取标题的核心内容（去除#、序号等）\n",
    "        \n",
    "        Args:\n",
    "            heading: 标题\n",
    "        \n",
    "        Returns:\n",
    "            str: 核心内容\n",
    "        \"\"\"\n",
    "        if not heading:\n",
    "            return \"\"\n",
    "        \n",
    "        # 去除首尾空格\n",
    "        content = heading.strip()\n",
    "        \n",
    "        # 去除开头的#号\n",
    "        if content.startswith('#'):\n",
    "            content = content[1:].strip()\n",
    "        \n",
    "        # 去除开头的序号（如 \"1. \"、\"2. \"等）\n",
    "        # 匹配模式：数字 + 点 + 空格\n",
    "        content = re.sub(r'^\\d+\\.\\s*', '', content)\n",
    "        \n",
    "        # 去除多余空格\n",
    "        content = re.sub(r'\\s+', ' ', content).strip()\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def _isHeadingMatch(self, heading1: str, heading2: str) -> bool:\n",
    "        \"\"\"\n",
    "        判断两个标题是否匹配（支持灵活匹配）\n",
    "        \n",
    "        Args:\n",
    "            heading1: 标题1\n",
    "            heading2: 标题2\n",
    "        \n",
    "        Returns:\n",
    "            bool: 是否匹配\n",
    "        \"\"\"\n",
    "        # 标准化两个标题\n",
    "        norm1 = self._normalizeHeading(heading1)\n",
    "        norm2 = self._normalizeHeading(heading2)\n",
    "        \n",
    "        # 提取核心内容\n",
    "        content1 = self._extractHeadingContent(norm1)\n",
    "        content2 = self._extractHeadingContent(norm2)\n",
    "        \n",
    "        # 完全匹配\n",
    "        if content1 == content2:\n",
    "            return True\n",
    "        \n",
    "        # 忽略大小写匹配\n",
    "        if content1.lower() == content2.lower():\n",
    "            return True\n",
    "        \n",
    "        # 去除所有空格后匹配\n",
    "        clean1 = re.sub(r'\\s+', '', content1)\n",
    "        clean2 = re.sub(r'\\s+', '', content2)\n",
    "        if clean1.lower() == clean2.lower():\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def extractContentByHeading(self, headingList: List[str], filePath: str, targetHeading: str) -> str:\n",
    "        \"\"\"\n",
    "        根据标题列表和指定标题提取内容\n",
    "        \n",
    "        Args:\n",
    "            headingList: 标题列表，例如 ['# Abstract', '# 1. Introduction', '# 2. Methods']\n",
    "            filePath: Markdown文件路径\n",
    "            targetHeading: 目标标题，例如 \"# 5. Conclusion\"\n",
    "        \n",
    "        Returns:\n",
    "            str: 提取的内容文本\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 读取文件内容\n",
    "            with open(filePath, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "            \n",
    "            # 按行分割内容\n",
    "            lines = content.split('\\n')\n",
    "            \n",
    "            # 找到目标标题在标题列表中的位置\n",
    "            targetIndex = self._findHeadingIndex(headingList, targetHeading)\n",
    "            if targetIndex == -1:\n",
    "                return f\"错误：在标题列表中未找到目标标题 '{targetHeading}'\"\n",
    "            \n",
    "            # 找到目标标题在文件中的位置\n",
    "            targetLineIndex = self._findHeadingInFile(lines, targetHeading)\n",
    "            if targetLineIndex == -1:\n",
    "                return f\"错误：在文件中未找到目标标题 '{targetHeading}'\"\n",
    "            \n",
    "            # 确定结束位置\n",
    "            if targetIndex == len(headingList) - 1:\n",
    "                # 如果是最后一个标题，提取到文件结束\n",
    "                endLineIndex = len(lines)\n",
    "            else:\n",
    "                # 找到下一个标题在文件中的位置\n",
    "                nextHeading = headingList[targetIndex + 1]\n",
    "                endLineIndex = self._findHeadingInFile(lines, nextHeading)\n",
    "                if endLineIndex == -1:\n",
    "                    return f\"错误：在文件中未找到下一个标题 '{nextHeading}'\"\n",
    "            \n",
    "            # 提取内容\n",
    "            extractedLines = lines[targetLineIndex:endLineIndex]\n",
    "            extractedContent = '\\n'.join(extractedLines)\n",
    "            \n",
    "            return extractedContent.strip()\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            return f\"错误：文件 '{filePath}' 不存在\"\n",
    "        except Exception as e:\n",
    "            return f\"错误：处理文件时发生异常 - {str(e)}\"\n",
    "    \n",
    "    def _findHeadingIndex(self, headingList: List[str], targetHeading: str) -> int:\n",
    "        \"\"\"\n",
    "        在标题列表中找到目标标题的索引（支持灵活匹配）\n",
    "        \n",
    "        Args:\n",
    "            headingList: 标题列表\n",
    "            targetHeading: 目标标题\n",
    "        \n",
    "        Returns:\n",
    "            int: 标题索引，未找到返回-1\n",
    "        \"\"\"\n",
    "        for i, heading in enumerate(headingList):\n",
    "            if self._isHeadingMatch(heading, targetHeading):\n",
    "                return i\n",
    "        return -1\n",
    "    \n",
    "    def _findHeadingInFile(self, lines: List[str], heading: str) -> int:\n",
    "        \"\"\"\n",
    "        在文件行中找到标题的位置（支持灵活匹配）\n",
    "        \n",
    "        Args:\n",
    "            lines: 文件行列表\n",
    "            heading: 要查找的标题\n",
    "        \n",
    "        Returns:\n",
    "            int: 行索引，未找到返回-1\n",
    "        \"\"\"\n",
    "        for i, line in enumerate(lines):\n",
    "            if self._isHeadingMatch(line, heading):\n",
    "                return i\n",
    "        return -1\n",
    "    \n",
    "    def extractContentByHeadingWithDebug(self, headingList: List[str], filePath: str, targetHeading: str) -> dict:\n",
    "        \"\"\"\n",
    "        带调试信息的提取内容方法\n",
    "        \n",
    "        Args:\n",
    "            headingList: 标题列表\n",
    "            filePath: Markdown文件路径\n",
    "            targetHeading: 目标标题\n",
    "        \n",
    "        Returns:\n",
    "            dict: 包含结果和调试信息的字典\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            'success': False,\n",
    "            'content': '',\n",
    "            'debug_info': {\n",
    "                'target_heading': targetHeading,\n",
    "                'target_index': -1,\n",
    "                'next_heading': '',\n",
    "                'start_line': -1,\n",
    "                'end_line': -1,\n",
    "                'total_lines': 0,\n",
    "                'error': ''\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # 读取文件内容\n",
    "            with open(filePath, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "            \n",
    "            lines = content.split('\\n')\n",
    "            result['debug_info']['total_lines'] = len(lines)\n",
    "            \n",
    "            # 找到目标标题在标题列表中的位置\n",
    "            targetIndex = self._findHeadingIndex(headingList, targetHeading)\n",
    "            result['debug_info']['target_index'] = targetIndex\n",
    "            \n",
    "            if targetIndex == -1:\n",
    "                result['debug_info']['error'] = f\"在标题列表中未找到目标标题 '{targetHeading}'\"\n",
    "                return result\n",
    "            \n",
    "            # 找到目标标题在文件中的位置\n",
    "            targetLineIndex = self._findHeadingInFile(lines, targetHeading)\n",
    "            result['debug_info']['start_line'] = targetLineIndex\n",
    "            \n",
    "            if targetLineIndex == -1:\n",
    "                result['debug_info']['error'] = f\"在文件中未找到目标标题 '{targetHeading}'\"\n",
    "                return result\n",
    "            \n",
    "            # 确定结束位置\n",
    "            if targetIndex == len(headingList) - 1:\n",
    "                # 最后一个标题\n",
    "                endLineIndex = len(lines)\n",
    "                result['debug_info']['next_heading'] = '文件结束'\n",
    "            else:\n",
    "                # 找到下一个标题\n",
    "                nextHeading = headingList[targetIndex + 1]\n",
    "                result['debug_info']['next_heading'] = nextHeading\n",
    "                endLineIndex = self._findHeadingInFile(lines, nextHeading)\n",
    "                \n",
    "                if endLineIndex == -1:\n",
    "                    result['debug_info']['error'] = f\"在文件中未找到下一个标题 '{nextHeading}'\"\n",
    "                    return result\n",
    "            \n",
    "            result['debug_info']['end_line'] = endLineIndex\n",
    "            \n",
    "            # 提取内容\n",
    "            extractedLines = lines[targetLineIndex:endLineIndex]\n",
    "            extractedContent = '\\n'.join(extractedLines)\n",
    "            \n",
    "            result['success'] = True\n",
    "            result['content'] = extractedContent.strip()\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            result['debug_info']['error'] = f\"文件 '{filePath}' 不存在\"\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            result['debug_info']['error'] = f\"处理文件时发生异常 - {str(e)}\"\n",
    "            return result\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"主函数，用于测试功能\"\"\"\n",
    "#     extractor = ContentExtractor()\n",
    "    \n",
    "#     # 测试数据\n",
    "#     headingList = [\n",
    "#         '# A Q-learning approach to the continuous control problem of robot inverted pendulum balancing',\n",
    "#         '# Corresponding Author:',\n",
    "#         '# A Q-learning approach to the continuous control problem of robot inverted pendulum balancing',\n",
    "#         '# Abstract',\n",
    "#         '# 1. Introduction',\n",
    "#         '# 2. Proposed approach and background',\n",
    "#         '# 3. Methodologies',\n",
    "#         '# 4. Results and discussion',\n",
    "#         '# 5. Conclusion',\n",
    "#         '# Acknowledgements',\n",
    "#         '# References'\n",
    "#     ]\n",
    "    \n",
    "#     filePath = \"/Users/xiaokong/task/2025/paper_vis/vis/md/2dbbabd2678ba74fcd9b08aadae975ae.md\"\n",
    "#     targetHeading = \"# 5. Conclusion\"\n",
    "    \n",
    "#     print(\"=== 测试内容提取功能 ===\")\n",
    "#     print(f\"目标标题: {targetHeading}\")\n",
    "#     print(f\"文件路径: {filePath}\")\n",
    "#     print()\n",
    "    \n",
    "#     # 测试基本功能\n",
    "#     content = extractor.extractContentByHeading(headingList, filePath, targetHeading)\n",
    "#     print(\"=== 提取的内容 ===\")\n",
    "#     print(content)\n",
    "#     print()\n",
    "    \n",
    "#     # 测试调试功能\n",
    "#     debugResult = extractor.extractContentByHeadingWithDebug(headingList, filePath, targetHeading)\n",
    "#     print(\"=== 调试信息 ===\")\n",
    "#     print(f\"成功: {debugResult['success']}\")\n",
    "#     print(f\"目标标题: {debugResult['debug_info']['target_heading']}\")\n",
    "#     print(f\"目标索引: {debugResult['debug_info']['target_index']}\")\n",
    "#     print(f\"下一个标题: {debugResult['debug_info']['next_heading']}\")\n",
    "#     print(f\"开始行: {debugResult['debug_info']['start_line']}\")\n",
    "#     print(f\"结束行: {debugResult['debug_info']['end_line']}\")\n",
    "#     print(f\"总行数: {debugResult['debug_info']['total_lines']}\")\n",
    "#     if debugResult['debug_info']['error']:\n",
    "#         print(f\"错误: {debugResult['debug_info']['error']}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd04e90",
   "metadata": {},
   "source": [
    "# 从markdown中找到所有模块的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbc09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "综合内容提取器\n",
    "整合标题规范化、标题映射和内容提取功能\n",
    "\n",
    "功能流程：\n",
    "1. 使用NormalizeHeadings.py获取清洗后的一级标题列表\n",
    "2. 使用TitleMappingLLM.py将标题映射到四个标准泳道\n",
    "3. 使用extractContentByHeading.py根据映射结果提取具体内容\n",
    "4. 返回按泳道组织的完整内容字典\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Dict, List\n",
    "from NormalizeHeadings import HeadingNormalizer\n",
    "from TitleMappingLLM import TitleMappingLLM\n",
    "from extractContentByHeading import ContentExtractor\n",
    "\n",
    "\n",
    "class ComprehensiveContentExtractor:\n",
    "    \"\"\"综合内容提取器\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"初始化综合提取器\"\"\"\n",
    "        self.heading_normalizer = HeadingNormalizer()\n",
    "        self.title_mapper = TitleMappingLLM()\n",
    "        self.content_extractor = ContentExtractor()\n",
    "    \n",
    "    def extract_comprehensive_content(self, markdown_file_path: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        综合内容提取主函数\n",
    "        \n",
    "        Args:\n",
    "            markdown_file_path: Markdown文件路径\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, str]: 按四个标准泳道组织的完整内容字典\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 步骤1: 获取清洗后的一级标题列表\n",
    "            print(\"步骤1: 获取清洗后的一级标题列表...\")\n",
    "            h1_headings = self.heading_normalizer.process_markdown_file(markdown_file_path)\n",
    "            \n",
    "            if not h1_headings:\n",
    "                print(\"错误: 未能提取到一级标题\")\n",
    "                return {}\n",
    "            \n",
    "            print(f\"提取到 {len(h1_headings)} 个一级标题:\")\n",
    "            for i, heading in enumerate(h1_headings, 1):\n",
    "                print(f\"  {i}. {heading}\")\n",
    "            print()\n",
    "            \n",
    "            # 步骤2: 将标题映射到四个标准泳道\n",
    "            print(\"步骤2: 将标题映射到四个标准泳道...\")\n",
    "            mapping_result = self.title_mapper.map_titles(h1_headings)\n",
    "            \n",
    "            if not mapping_result:\n",
    "                print(\"错误: 标题映射失败\")\n",
    "                return {}\n",
    "            \n",
    "            print(\"映射结果:\")\n",
    "            for lane, titles in mapping_result.items():\n",
    "                print(f\"  {lane}: {titles}\")\n",
    "            print()\n",
    "            \n",
    "            # 步骤3: 根据映射结果提取具体内容\n",
    "            print(\"步骤3: 提取各泳道的具体内容...\")\n",
    "            final_result = {}\n",
    "            \n",
    "            for lane_name, mapped_titles in mapping_result.items():\n",
    "                print(f\"处理泳道: {lane_name}\")\n",
    "                lane_content = \"\"\n",
    "                \n",
    "                for title in mapped_titles:\n",
    "                    print(f\"  提取标题: {title}\")\n",
    "                    content = self.content_extractor.extractContentByHeading(\n",
    "                        h1_headings, markdown_file_path, title\n",
    "                    )\n",
    "                    \n",
    "                    # 检查是否提取成功\n",
    "                    if content.startswith(\"错误：\"):\n",
    "                        print(f\"    警告: {content}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # 拼接内容\n",
    "                    if lane_content:\n",
    "                        lane_content += \"\\n\\n\" + content\n",
    "                    else:\n",
    "                        lane_content = content\n",
    "                    \n",
    "                    print(f\"    成功提取 {len(content)} 个字符\")\n",
    "                \n",
    "                final_result[lane_name] = lane_content\n",
    "                print(f\"  泳道 '{lane_name}' 总内容长度: {len(lane_content)} 字符\")\n",
    "                print()\n",
    "            \n",
    "            print(\"=== 综合内容提取完成 ===\")\n",
    "            return final_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"综合内容提取过程中发生异常: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def extract_content_with_summary(self, markdown_file_path: str) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        带摘要信息的综合内容提取\n",
    "        \n",
    "        Args:\n",
    "            markdown_file_path: Markdown文件路径\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, any]: 包含内容和摘要信息的字典\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 获取清洗后的一级标题列表\n",
    "            h1_headings = self.heading_normalizer.process_markdown_file(markdown_file_path)\n",
    "            \n",
    "            if not h1_headings:\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'error': '未能提取到一级标题',\n",
    "                    'content': {}\n",
    "                }\n",
    "            \n",
    "            # 将标题映射到四个标准泳道\n",
    "            mapping_result = self.title_mapper.map_titles(h1_headings)\n",
    "            \n",
    "            if not mapping_result:\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'error': '标题映射失败',\n",
    "                    'content': {}\n",
    "                }\n",
    "            \n",
    "            # 提取具体内容\n",
    "            final_result = {}\n",
    "            extraction_stats = {}\n",
    "            \n",
    "            for lane_name, mapped_titles in mapping_result.items():\n",
    "                lane_content = \"\"\n",
    "                extraction_stats[lane_name] = {\n",
    "                    'titles_count': len(mapped_titles),\n",
    "                    'titles': mapped_titles,\n",
    "                    'successful_extractions': 0,\n",
    "                    'failed_extractions': 0\n",
    "                }\n",
    "                \n",
    "                for title in mapped_titles:\n",
    "                    content = self.content_extractor.extractContentByHeading(\n",
    "                        h1_headings, markdown_file_path, title\n",
    "                    )\n",
    "                    \n",
    "                    if content.startswith(\"错误：\"):\n",
    "                        extraction_stats[lane_name]['failed_extractions'] += 1\n",
    "                        continue\n",
    "                    \n",
    "                    extraction_stats[lane_name]['successful_extractions'] += 1\n",
    "                    \n",
    "                    if lane_content:\n",
    "                        lane_content += \"\\n\\n\" + content\n",
    "                    else:\n",
    "                        lane_content = content\n",
    "                \n",
    "                final_result[lane_name] = lane_content\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'content': final_result,\n",
    "                'summary': {\n",
    "                    'total_h1_headings': len(h1_headings),\n",
    "                    'h1_headings': h1_headings,\n",
    "                    'mapping_result': mapping_result,\n",
    "                    'extraction_stats': extraction_stats,\n",
    "                    'file_path': markdown_file_path\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': f'处理过程中发生异常: {e}',\n",
    "                'content': {}\n",
    "            }\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"主函数，用于测试综合内容提取功能\"\"\"\n",
    "    print(\"=== 综合内容提取器测试 ===\")\n",
    "    \n",
    "    # 创建综合提取器实例\n",
    "    extractor = ComprehensiveContentExtractor()\n",
    "    \n",
    "    # 测试文件路径（请根据实际情况修改）\n",
    "    test_file_path = \"/Users/xiaokong/task/2025/paper_vis/vis/md/2dbbabd2678ba74fcd9b08aadae975ae.md\"\n",
    "    \n",
    "    # 检查文件是否存在\n",
    "    if not os.path.exists(test_file_path):\n",
    "        print(f\"错误: 测试文件不存在: {test_file_path}\")\n",
    "        print(\"请修改 test_file_path 变量为实际存在的Markdown文件路径\")\n",
    "        return\n",
    "    \n",
    "    print(f\"测试文件: {test_file_path}\")\n",
    "    print()\n",
    "    \n",
    "    # 执行综合内容提取\n",
    "    result = extractor.extract_comprehensive_content(test_file_path)\n",
    "    \n",
    "    if result:\n",
    "        print(\"=== 最终结果 ===\")\n",
    "        for lane_name, content in result.items():\n",
    "            print(f\"\\n泳道: {lane_name}\")\n",
    "            print(f\"内容长度: {len(content)} 字符\")\n",
    "            print(f\"内容预览: {content}...\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"内容提取失败\")\n",
    "    \n",
    "    print(\"\\n=== 测试完成 ===\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de927faf",
   "metadata": {},
   "source": [
    "# 匹配图表位置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aaadde",
   "metadata": {},
   "source": [
    "{\n",
    "  \"Context & Related Work\": [\n",
    "    {\n",
    "      \"figure_id\": \"b357bf6b985700391e95e23414e4b79c293d3fc25bdec3f58908049d50871fa7\", \n",
    "      \"figure_caption\": \"....\",\n",
    "      \"reference_text\": [\"....\",\"...\"，”...“] \n",
    "    }\n",
    "  ],\n",
    "  \"Methodology & Setup\": [\n",
    "    {\n",
    "      \"figure_id\": \"80b3149cfde69122d789174087de284b1b7ca2a42efb05472127cf3a3416e08e\",\n",
    "      \"figure_caption\": \"....\",\n",
    "      \"reference_text\": [\"....\"]\n",
    "    },\n",
    "    // ... 更多图表\n",
    "  ],\n",
    "  \"Results & Analysis\": [],\n",
    "  \"Conclusion\": []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f2127d",
   "metadata": {},
   "source": [
    "1. 通过ComprehensiveContentExtractor获取到最终需要抽取点的原始文本\n",
    "2. 通过merge_data得到mergedata，执行get_figure得到图表数据\n",
    "3. 遍历每个caption去匹配到泳道\n",
    "4. 形成figure_map数据以作为图表可视化数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e188082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sumy\n",
      "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: docopt<0.7,>=0.6.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from sumy) (0.6.2)\n",
      "Collecting breadability>=0.1.20 (from sumy)\n",
      "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from sumy) (2.32.5)\n",
      "Collecting pycountry>=18.2.23 (from sumy)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: nltk>=3.0.2 in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from sumy) (3.9.1)\n",
      "Requirement already satisfied: chardet in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
      "Requirement already satisfied: lxml>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from breadability>=0.1.20->sumy) (5.3.1)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from nltk>=3.0.2->sumy) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from requests>=2.7.0->sumy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from requests>=2.7.0->sumy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from requests>=2.7.0->sumy) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from requests>=2.7.0->sumy) (2025.1.31)\n",
      "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
      "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: breadability\n",
      "\u001b[33m  DEPRECATION: Building 'breadability' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'breadability'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for breadability (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21795 sha256=44e31405e7c6bf59cc3539360dc277dff0ffb75e3c3470aed4e3dea5a268db9f\n",
      "  Stored in directory: /Users/xiaokong/Library/Caches/pip/wheels/32/99/64/59305409cacd03aa03e7bddf31a9db34b1fa7033bd41972662\n",
      "Successfully built breadability\n",
      "Installing collected packages: pycountry, breadability, sumy\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [sumy]\n",
      "\u001b[1A\u001b[2KSuccessfully installed breadability-0.1.20 pycountry-24.6.1 sumy-0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sumy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d3e8e",
   "metadata": {},
   "source": [
    "# 总调度器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230e3a36",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea142fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96382db0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
