{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4dd9f06",
   "metadata": {},
   "source": [
    "# æµ‹è¯•å¹¶å‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6eadfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "å¼€å§‹æµ‹è¯• 10 ä¸ªå¹¶å‘è¯·æ±‚\n",
      "============================================================\n",
      "\n",
      "[13:35:17.543] è¯·æ±‚ #1 å¼€å§‹\n",
      "[13:35:17.543] è¯·æ±‚ #2 å¼€å§‹\n",
      "[13:35:17.543] è¯·æ±‚ #3 å¼€å§‹\n",
      "[13:35:17.543] è¯·æ±‚ #4 å¼€å§‹\n",
      "[13:35:17.543] è¯·æ±‚ #5 å¼€å§‹\n",
      "[13:35:17.544] è¯·æ±‚ #6 å¼€å§‹\n",
      "[13:35:17.544] è¯·æ±‚ #7 å¼€å§‹\n",
      "[13:35:17.544] è¯·æ±‚ #8 å¼€å§‹\n",
      "[13:35:17.544] è¯·æ±‚ #9 å¼€å§‹\n",
      "[13:35:17.544] è¯·æ±‚ #10 å¼€å§‹\n",
      "[13:35:19.495] è¯·æ±‚ #6 æˆåŠŸ (è€—æ—¶: 1.95ç§’)\n",
      "[13:35:19.496] è¯·æ±‚ #4 æˆåŠŸ (è€—æ—¶: 1.95ç§’)\n",
      "[13:35:19.500] è¯·æ±‚ #9 æˆåŠŸ (è€—æ—¶: 1.96ç§’)\n",
      "[13:35:19.502] è¯·æ±‚ #2 æˆåŠŸ (è€—æ—¶: 1.96ç§’)\n",
      "[13:35:19.504] è¯·æ±‚ #3 æˆåŠŸ (è€—æ—¶: 1.96ç§’)\n",
      "[13:35:19.509] è¯·æ±‚ #7 æˆåŠŸ (è€—æ—¶: 1.97ç§’)\n",
      "[13:35:19.509] è¯·æ±‚ #5 æˆåŠŸ (è€—æ—¶: 1.97ç§’)\n",
      "[13:35:19.568] è¯·æ±‚ #10 æˆåŠŸ (è€—æ—¶: 2.02ç§’)\n",
      "[13:35:19.568] è¯·æ±‚ #8 æˆåŠŸ (è€—æ—¶: 2.02ç§’)\n",
      "[13:35:19.869] è¯·æ±‚ #1 æˆåŠŸ (è€—æ—¶: 2.33ç§’)\n",
      "\n",
      "============================================================\n",
      "æµ‹è¯•å®Œæˆï¼æ€»è€—æ—¶: 2.33ç§’\n",
      "============================================================\n",
      "\n",
      "æˆåŠŸ: 10/10\n",
      "å¤±è´¥: 0/10\n",
      "å¹³å‡å“åº”æ—¶é—´: 2.01ç§’\n",
      "\n",
      "å¦‚æœæ€»è€—æ—¶æ¥è¿‘å¹³å‡å“åº”æ—¶é—´ï¼Œè¯´æ˜æ”¯æŒçœŸæ­£çš„å¹¶å‘\n",
      "å¦‚æœæ€»è€—æ—¶æ¥è¿‘å¹³å‡å“åº”æ—¶é—´Ã—è¯·æ±‚æ•°ï¼Œè¯´æ˜æ˜¯ä¸²è¡Œæ‰§è¡Œ\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# DeepSeek APIé…ç½®\n",
    "API_KEY = \"sk-8de35978ccec41e39e2b9ebfc90b7aa1\"\n",
    "API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "MODEL = \"deepseek-chat\"\n",
    "\n",
    "async def call_deepseek_api(session, request_id):\n",
    "    \"\"\"å¼‚æ­¥è°ƒç”¨DeepSeek API\"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": f\"è¿™æ˜¯æµ‹è¯•è¯·æ±‚ #{request_id}ï¼Œè¯·ç®€å•å›å¤æ”¶åˆ°ã€‚\"}\n",
    "        ],\n",
    "        \"max_tokens\": 50\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    print(f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] è¯·æ±‚ #{request_id} å¼€å§‹\")\n",
    "    \n",
    "    try:\n",
    "        async with session.post(API_URL, json=payload, headers=headers) as response:\n",
    "            result = await response.json()\n",
    "            end_time = time.time()\n",
    "            elapsed = end_time - start_time\n",
    "            \n",
    "            if response.status == 200:\n",
    "                print(f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] è¯·æ±‚ #{request_id} æˆåŠŸ (è€—æ—¶: {elapsed:.2f}ç§’)\")\n",
    "                return {\"id\": request_id, \"status\": \"success\", \"time\": elapsed, \"response\": result}\n",
    "            else:\n",
    "                print(f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] è¯·æ±‚ #{request_id} å¤±è´¥: {response.status}\")\n",
    "                return {\"id\": request_id, \"status\": \"failed\", \"time\": elapsed, \"error\": result}\n",
    "    except Exception as e:\n",
    "        end_time = time.time()\n",
    "        elapsed = end_time - start_time\n",
    "        print(f\"[{datetime.now().strftime('%H:%M:%S.%f')[:-3]}] è¯·æ±‚ #{request_id} å¼‚å¸¸: {str(e)}\")\n",
    "        return {\"id\": request_id, \"status\": \"error\", \"time\": elapsed, \"error\": str(e)}\n",
    "\n",
    "async def test_concurrent_requests(num_requests=5):\n",
    "    \"\"\"æµ‹è¯•å¹¶å‘è¯·æ±‚\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"å¼€å§‹æµ‹è¯• {num_requests} ä¸ªå¹¶å‘è¯·æ±‚\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        # åˆ›å»º5ä¸ªå¹¶å‘ä»»åŠ¡\n",
    "        tasks = [call_deepseek_api(session, i+1) for i in range(num_requests)]\n",
    "        # åŒæ—¶æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡\n",
    "        results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"æµ‹è¯•å®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}ç§’\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # ç»Ÿè®¡ç»“æœ\n",
    "    success_count = sum(1 for r in results if r[\"status\"] == \"success\")\n",
    "    failed_count = sum(1 for r in results if r[\"status\"] != \"success\")\n",
    "    avg_time = sum(r[\"time\"] for r in results) / len(results)\n",
    "    \n",
    "    print(f\"æˆåŠŸ: {success_count}/{num_requests}\")\n",
    "    print(f\"å¤±è´¥: {failed_count}/{num_requests}\")\n",
    "    print(f\"å¹³å‡å“åº”æ—¶é—´: {avg_time:.2f}ç§’\")\n",
    "    print(f\"\\nå¦‚æœæ€»è€—æ—¶æ¥è¿‘å¹³å‡å“åº”æ—¶é—´ï¼Œè¯´æ˜æ”¯æŒçœŸæ­£çš„å¹¶å‘\")\n",
    "    print(f\"å¦‚æœæ€»è€—æ—¶æ¥è¿‘å¹³å‡å“åº”æ—¶é—´Ã—è¯·æ±‚æ•°ï¼Œè¯´æ˜æ˜¯ä¸²è¡Œæ‰§è¡Œ\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# è¿è¡Œæµ‹è¯•\n",
    "results = await test_concurrent_requests(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6a03c4",
   "metadata": {},
   "source": [
    "# æµ‹è¯•è§„åˆ™æŠ½å–æ‘˜è¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "587d48e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æµ‹è¯•ä¿®å¤åçš„abstractæå–å‡½æ•°:\n",
      "============================================================\n",
      "æµ‹è¯•æ–‡ä»¶1: /Users/xiaokong/task/2025/paper_vis/vis/md/af705d1369467b0aa55cb59354a84a0e.md\n",
      "æ‰¾åˆ°abstractæ ‡é¢˜åœ¨ç¬¬14è¡Œ: # ---- A B S T R A C T  ----\n",
      "True\n",
      "âœ“ æˆåŠŸæå–Abstractå†…å®¹:\n",
      "Keywords: Brain-inspired continual learning Context similarity assessment Neuronal discriminative expansion Neuronal selective reuse Sparse spiking neural networks  \n",
      "\n",
      "Biological brains have the capability to adaptively coordinate relevant neuronal populations based on the task context to learn continuously changing tasks in real-world environments. However, existing spiking neural network-based continual learning algorithms treat each task equally, ignoring the guiding role of different task similarity associations for network learning, which limits knowledge utilization efficiency. Inspired by the context-dependent plasticity mechanism of the brain, we propose a Similarity-based Context Aware Spiking Neural Network (SCA-SNN) continual learning algorithm to efficiently accomplish task incremental learning and class incremental learning. Based on contextual similarity across tasks, the SCA-SNN model can adaptively reuse neurons from previous tasks that are beneficial for new tasks (the more similar, the more neurons are reused) and flexibly expand new neurons for the new task (the more similar, the fewer neurons are expanded). Selective reuse and discriminative expansion significantly improve the utilization of previous knowledge and reduce energy consumption. Extensive experimental results on CIFAR100, ImageNet generalized datasets, and FMNIST-MNIST, SVHN-CIFAR100 mixed datasets show that our SCA-SNN model achieves superior performance compared to both SNN-based and DNN-based continual learning algorithms. Additionally, our algorithm has the capability to adaptively select similar groups of neurons for related tasks, offering a promising approach to enhancing the biological interpretability of efficient continual learning.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_abstract_from_md(file_path):\n",
    "    \"\"\"\n",
    "    ä»Markdownæ–‡ä»¶ä¸­æå–Abstractéƒ¨åˆ†çš„æ­£æ–‡å†…å®¹\n",
    "    \n",
    "    å‚æ•°:\n",
    "        file_path: str æˆ– Pathå¯¹è±¡,æŒ‡å‘.mdæ–‡ä»¶çš„è·¯å¾„\n",
    "    \n",
    "    è¿”å›:\n",
    "        tuple: (bool, str) ç¬¬ä¸€ä¸ªå…ƒç´ è¡¨ç¤ºæ˜¯å¦æ‰¾åˆ°æ‘˜è¦,ç¬¬äºŒä¸ªå…ƒç´ æ˜¯æ‘˜è¦æ–‡æœ¬\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # è¯»å–æ–‡ä»¶å†…å®¹\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # æŒ‰è¡Œåˆ†å‰²\n",
    "        lines = content.split('\\n')\n",
    "        \n",
    "        # æ›´çµæ´»çš„abstractåŒ¹é…æ¨¡å¼\n",
    "        def is_abstract_header(line):\n",
    "            \"\"\"åˆ¤æ–­ä¸€è¡Œæ˜¯å¦æ˜¯abstractæ ‡é¢˜\"\"\"\n",
    "            # å»é™¤é¦–å°¾ç©ºç™½\n",
    "            stripped = line.strip()\n",
    "            \n",
    "            # å¿…é¡»ä»¥#å¼€å¤´\n",
    "            if not stripped.startswith('#'):\n",
    "                return False\n",
    "            \n",
    "            # å»æ‰#å·å’Œåé¢çš„ç©ºç™½å­—ç¬¦\n",
    "            after_hash = stripped[1:].strip()\n",
    "            \n",
    "            # å¦‚æœ#åé¢æ²¡æœ‰å†…å®¹,ä¸æ˜¯æœ‰æ•ˆæ ‡é¢˜\n",
    "            if not after_hash:\n",
    "                return False\n",
    "            \n",
    "            # å»æ‰æ‰€æœ‰ç©ºæ ¼ã€æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦,åªä¿ç•™å­—æ¯\n",
    "            # è¿™æ ·å¯ä»¥åŒ¹é… \"A B S T R A C T\", \"Abstract:\", \"Abstract.\" ç­‰å„ç§å½¢å¼\n",
    "            cleaned = re.sub(r'[^a-zA-Z\\u4e00-\\u9fff]', '', after_hash)\n",
    "            \n",
    "            # è½¬ä¸ºå°å†™è¿›è¡Œæ¯”è¾ƒ\n",
    "            normalized = cleaned.lower()\n",
    "            \n",
    "            # æ£€æŸ¥æ˜¯å¦åŒ¹é…abstractçš„å„ç§å½¢å¼\n",
    "            abstract_variants = [\n",
    "                'abstract',\n",
    "                'abstracts',\n",
    "                'æ‘˜è¦',\n",
    "            ]\n",
    "            \n",
    "            # ç›´æ¥æ£€æŸ¥æ˜¯å¦åŒ¹é…\n",
    "            if normalized in abstract_variants:\n",
    "                return True\n",
    "            \n",
    "            return False\n",
    "        \n",
    "        # æŸ¥æ‰¾abstractå¼€å§‹ä½ç½®\n",
    "        abstract_start = -1\n",
    "        for i, line in enumerate(lines):\n",
    "            if is_abstract_header(line):\n",
    "                abstract_start = i\n",
    "                print(f\"æ‰¾åˆ°abstractæ ‡é¢˜åœ¨ç¬¬{i+1}è¡Œ: {line.strip()}\")\n",
    "                break\n",
    "        \n",
    "        # å¦‚æœæ²¡æ‰¾åˆ°abstractæ ‡é¢˜,è¿”å›(False, \"\")\n",
    "        if abstract_start == -1:\n",
    "            print(\"æœªæ‰¾åˆ°abstractæ ‡é¢˜\")\n",
    "            return (False, \"\")\n",
    "        \n",
    "        # æŸ¥æ‰¾ä¸‹ä¸€ä¸ªä¸€çº§æ ‡é¢˜(abstractç»“æŸä½ç½®)\n",
    "        abstract_end = len(lines)\n",
    "        for i in range(abstract_start + 1, len(lines)):\n",
    "            # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸€çº§æ ‡é¢˜(ä»¥å•ä¸ª#å¼€å¤´,åé¢è·Ÿç©ºæ ¼å’Œéç©ºç™½å­—ç¬¦)\n",
    "            stripped = lines[i].strip()\n",
    "            if stripped.startswith('#') and not stripped.startswith('##'):\n",
    "                # ç¡®ä¿#åé¢æœ‰å†…å®¹\n",
    "                after_hash = stripped[1:].strip()\n",
    "                if after_hash:\n",
    "                    abstract_end = i\n",
    "                    break\n",
    "        \n",
    "        # æå–abstractå†…å®¹(è·³è¿‡æ ‡é¢˜è¡Œæœ¬èº«)\n",
    "        abstract_lines = lines[abstract_start + 1:abstract_end]\n",
    "        \n",
    "        # å»é™¤é¦–å°¾ç©ºè¡Œ\n",
    "        while abstract_lines and not abstract_lines[0].strip():\n",
    "            abstract_lines.pop(0)\n",
    "        while abstract_lines and not abstract_lines[-1].strip():\n",
    "            abstract_lines.pop()\n",
    "        \n",
    "        # åˆå¹¶ä¸ºæ–‡æœ¬\n",
    "        abstract_text = '\\n'.join(abstract_lines).strip()\n",
    "        \n",
    "        # æ£€æŸ¥æ‘˜è¦æ–‡æœ¬çš„å•è¯æ•°æ˜¯å¦å¤§äº30\n",
    "        word_count = len(abstract_text.split())\n",
    "        if word_count <= 30:\n",
    "            print(f\"æ‘˜è¦æ–‡æœ¬å•è¯æ•°ä¸è¶³: {word_count} ä¸ªå•è¯ (éœ€è¦å¤§äº30ä¸ª)\")\n",
    "            return (False, \"\")\n",
    "        \n",
    "        return (True, abstract_text)\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"é”™è¯¯:æ–‡ä»¶æœªæ‰¾åˆ° - {file_path}\")\n",
    "        return (False, \"\")\n",
    "    except Exception as e:\n",
    "        print(f\"é”™è¯¯:å¤„ç†æ–‡ä»¶æ—¶å‡ºç°å¼‚å¸¸ - {str(e)}\")\n",
    "        return (False, \"\")\n",
    "\n",
    "\n",
    "# æµ‹è¯•å‡½æ•°\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"æµ‹è¯•ä¿®å¤åçš„abstractæå–å‡½æ•°:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # æµ‹è¯•ç¬¬ä¸€ä¸ªæ–‡ä»¶\n",
    "    test_file1 = \"/Users/xiaokong/task/2025/paper_vis/vis/md/af705d1369467b0aa55cb59354a84a0e.md\"\n",
    "    print(f\"æµ‹è¯•æ–‡ä»¶1: {test_file1}\")\n",
    "    found, abstract1 = extract_abstract_from_md(test_file1)\n",
    "    if found:\n",
    "        print(found)\n",
    "        print(\"âœ“ æˆåŠŸæå–Abstractå†…å®¹:\")\n",
    "        print(abstract1)\n",
    "    else:\n",
    "        print(found)\n",
    "        print(\"âœ— æœªæ‰¾åˆ°Abstractå†…å®¹\")\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba024aa",
   "metadata": {},
   "source": [
    "# LLMæŠ½å–æ‘˜è¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62b04926",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qy/w56f01vd5h35lc_7plwj_k_r0000gn/T/ipykernel_1880/74677222.py:43: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  @validator('background_problem', 'method_approach', 'result', 'conclusion_contribution')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§ª å¼€å§‹æµ‹è¯•æ‘˜è¦è¯­æ­¥åˆ†æç³»ç»Ÿ\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ æµ‹è¯•æ–‡ä»¶ 1: af705d1369467b0aa55cb59354a84a0e.md\n",
      "\n",
      "================================================================================\n",
      "ğŸ” å¼€å§‹åˆ†ææ–‡ä»¶: /Users/xiaokong/task/2025/paper_vis/vis/md/af705d1369467b0aa55cb59354a84a0e.md\n",
      "================================================================================\n",
      "æ‰¾åˆ°abstractæ ‡é¢˜åœ¨ç¬¬14è¡Œ: # ---- A B S T R A C T  ----\n",
      "âœ… è§„åˆ™æå–æˆåŠŸï¼Œä½¿ç”¨æ‘˜è¦æ–‡æœ¬ (é•¿åº¦: 1752 å­—ç¬¦)\n",
      "\n",
      "ğŸ¤– å¼€å§‹LLMè¯­æ­¥åˆ†æ...\n",
      "ğŸ”„ LLMè°ƒç”¨å°è¯• 1/3\n",
      "ğŸ“ LLMåŸå§‹å›å¤: {\n",
      "    \"Background/Problem\": \"Biological brains adaptively learn tasks, but current spiking neural networks ignore task similarity, limiting knowledge reuse efficiency in continual learning.\",\n",
      "    \"Met...\n",
      "âœ… JSONè§£ææˆåŠŸ\n",
      "âœ… PydanticéªŒè¯æˆåŠŸ\n",
      "\n",
      "âœ… åˆ†æå®Œæˆï¼è€—æ—¶: 7.19ç§’\n",
      "ğŸ“Š è¯­æ­¥åˆ†æç»“æœ:\n",
      "   â€¢ èƒŒæ™¯/é—®é¢˜: Biological brains adaptively learn tasks, but current spiking neural networks ignore task similarity, limiting knowledge reuse efficiency in continual learning.\n",
      "   â€¢ æ–¹æ³•/é€”å¾„: We propose SCA-SNN, which adaptively reuses or expands neurons based on contextual task similarity to improve knowledge utilization and reduce energy.\n",
      "   â€¢ ç»“æœ: Experiments on multiple datasets show SCA-SNN outperforms existing SNN and DNN continual learning methods in task and class incremental learning.\n",
      "   â€¢ ç»“è®º/è´¡çŒ®: Our algorithm enhances biological interpretability and offers an efficient, adaptive approach to continual learning with selective neuron reuse and expansion.\n",
      "\n",
      "â³ ç­‰å¾…2ç§’åå¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶...\n",
      "\n",
      "ğŸ“ æµ‹è¯•æ–‡ä»¶ 2: 3791465d4e18e4033b5c7bd322c44df2.md\n",
      "\n",
      "================================================================================\n",
      "ğŸ” å¼€å§‹åˆ†ææ–‡ä»¶: /Users/xiaokong/task/2025/paper_vis/vis/md/3791465d4e18e4033b5c7bd322c44df2.md\n",
      "================================================================================\n",
      "æœªæ‰¾åˆ°abstractæ ‡é¢˜\n",
      "âš ï¸ è§„åˆ™æå–å¤±è´¥ï¼Œä½¿ç”¨å‰5000å­—ç¬¦\n",
      "ğŸ“„ ä½¿ç”¨å‰5000å­—ç¬¦ (å®é™…é•¿åº¦: 5000 å­—ç¬¦)\n",
      "\n",
      "ğŸ¤– å¼€å§‹LLMè¯­æ­¥åˆ†æ...\n",
      "ğŸ”„ LLMè°ƒç”¨å°è¯• 1/3\n",
      "ğŸ“ LLMåŸå§‹å›å¤: {\n",
      "    \"Background/Problem\": \"Cognitive decline in Alzheimer's disease is highly variable, and current biomarkers like amyloid-beta and tau explain only 20-40% of cognitive impairment variance.\",\n",
      "    \"...\n",
      "âœ… JSONè§£ææˆåŠŸ\n",
      "âœ… PydanticéªŒè¯æˆåŠŸ\n",
      "\n",
      "âœ… åˆ†æå®Œæˆï¼è€—æ—¶: 7.22ç§’\n",
      "ğŸ“Š è¯­æ­¥åˆ†æç»“æœ:\n",
      "   â€¢ èƒŒæ™¯/é—®é¢˜: Cognitive decline in Alzheimer's disease is highly variable, and current biomarkers like amyloid-beta and tau explain only 20-40% of cognitive impairment variance.\n",
      "   â€¢ æ–¹æ³•/é€”å¾„: We performed CSF proteomics on 3,397 individuals from six cohorts and used machine learning to derive the CSF YWHAG:NPTX2 synapse protein ratio.\n",
      "   â€¢ ç»“æœ: CSF YWHAG:NPTX2 explained significant additional variance in cognitive impairment and predicted conversion to mild cognitive impairment and dementia over 15 years.\n",
      "   â€¢ ç»“è®º/è´¡çŒ®: CSF YWHAG:NPTX2 is a robust prognostic biomarker for cognitive resilience versus Alzheimer's onset and progression, highlighting synapse dysfunction as a core driver.\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ˆ æµ‹è¯•ç»“æœæ±‡æ€»\n",
      "================================================================================\n",
      "âœ… æˆåŠŸ: 2/2\n",
      "âŒ å¤±è´¥: 0/2\n"
     ]
    }
   ],
   "source": [
    "# LLMæ‘˜è¦è¯­æ­¥æå–ç³»ç»Ÿ\n",
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import time\n",
    "from typing import Optional, Dict, Any\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from pathlib import Path\n",
    "\n",
    "# DeepSeek APIé…ç½®\n",
    "API_KEY = \"sk-8de35978ccec41e39e2b9ebfc90b7aa1\"\n",
    "API_URL = \"https://api.deepseek.com/v1/chat/completions\"\n",
    "MODEL = \"deepseek-chat\"\n",
    "\n",
    "# Pydantic Schemaå®šä¹‰\n",
    "class AbstractSteps(BaseModel):\n",
    "    \"\"\"å­¦æœ¯æ‘˜è¦å››æ­¥è¯­æ­¥ç»“æ„\"\"\"\n",
    "    background_problem: str = Field(\n",
    "        ..., \n",
    "        alias=\"Background/Problem\",\n",
    "        description=\"èƒŒæ™¯/é—®é¢˜æè¿°ï¼Œä¸è¶…è¿‡35ä¸ªè‹±æ–‡å•è¯\",\n",
    "        max_length=200\n",
    "    )\n",
    "    method_approach: str = Field(\n",
    "        ..., \n",
    "        alias=\"Method/Approach\", \n",
    "        description=\"æ–¹æ³•/é€”å¾„ï¼Œä¸è¶…è¿‡35ä¸ªè‹±æ–‡å•è¯\",\n",
    "        max_length=200\n",
    "    )\n",
    "    result: str = Field(\n",
    "        ..., \n",
    "        alias=\"Result\",\n",
    "        description=\"ç»“æœï¼Œä¸è¶…è¿‡35ä¸ªè‹±æ–‡å•è¯\", \n",
    "        max_length=200\n",
    "    )\n",
    "    conclusion_contribution: str = Field(\n",
    "        ..., \n",
    "        alias=\"Conclusion/Contribution\",\n",
    "        description=\"ç»“è®º/è´¡çŒ®ï¼Œä¸è¶…è¿‡35ä¸ªè‹±æ–‡å•è¯\",\n",
    "        max_length=200\n",
    "    )\n",
    "    \n",
    "    @validator('background_problem', 'method_approach', 'result', 'conclusion_contribution')\n",
    "    def validate_word_count(cls, v):\n",
    "        \"\"\"éªŒè¯æ¯ä¸ªå­—æ®µä¸è¶…è¿‡35ä¸ªè‹±æ–‡å•è¯\"\"\"\n",
    "        word_count = len(v.split())\n",
    "        if word_count > 35:\n",
    "            raise ValueError(f\"å­—æ®µè¶…è¿‡35ä¸ªå•è¯é™åˆ¶: {word_count} ä¸ªå•è¯\")\n",
    "        return v\n",
    "    \n",
    "    class Config:\n",
    "        allow_population_by_field_name = True\n",
    "        json_encoders = {\n",
    "            str: lambda v: v.strip()\n",
    "        }\n",
    "\n",
    "# ç³»ç»Ÿæç¤ºè¯\n",
    "SYSTEM_PROMPT = \"\"\"You are a highly specialized expert in academic text analysis and structured data extraction. Your **sole task** is to accurately identify and extract the four standard rhetorical steps of the academic abstract from the provided text, and output the result in a **strictly valid JSON format**.\n",
    "\n",
    "**[Core Task & Output]**\n",
    "1. The input text might be a **clean abstract** OR a **document segment** (including metadata, title, authors, and the abstract).\n",
    "2. If the text is a segment, you **must first locate the Abstract** based on its semantic features (a condensed summary of background, method, and results), and **ignore all surrounding noise**.\n",
    "3. Decompose the identified Abstract content into the following four standard rhetorical steps: Background/Problem, Method/Approach, Result, Conclusion/Contribution.\n",
    "\n",
    "**[Format Requirements]**\n",
    "1. **MUST** output a JSON object that strictly conforms to the provided schema.\n",
    "2. **ABSOLUTELY DO NOT** output any introductory text, explanations, or text outside the raw JSON object.\n",
    "3. Each summary **MUST NOT exceed 35 English words**, focusing on high-level summarization.\"\"\"\n",
    "\n",
    "# ç”¨æˆ·æç¤ºè¯æ¨¡æ¿\n",
    "USER_PROMPT_TEMPLATE = \"\"\"Please analyze the text provided below. And decompose its content into the four standard rhetorical steps.\n",
    "\n",
    "**[Text to Analyze]**\n",
    "{text}\n",
    "\n",
    "**[Expected JSON Schema]**\n",
    "Please adhere strictly to this JSON structure for your output:\n",
    "\n",
    "{{\n",
    "    \"Background/Problem\": \"The concise English summary for this step, no more than 35 words.\",\n",
    "    \"Method/Approach\": \"The concise English summary for this step, no more than 35 words.\",\n",
    "    \"Result\": \"The concise English summary for this step, no more than 35 words.\",\n",
    "    \"Conclusion/Contribution\": \"The concise English summary for this step, no more than 35 words.\"\n",
    "}}\"\"\"\n",
    "\n",
    "async def call_deepseek_api(text: str, max_retries: int = 2) -> Optional[AbstractSteps]:\n",
    "    \"\"\"\n",
    "    è°ƒç”¨DeepSeek APIè¿›è¡Œæ‘˜è¦è¯­æ­¥æå–\n",
    "    \n",
    "    Args:\n",
    "        text: è¦åˆ†æçš„æ–‡æœ¬\n",
    "        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°\n",
    "    \n",
    "    Returns:\n",
    "        AbstractStepså¯¹è±¡æˆ–None\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    user_prompt = USER_PROMPT_TEMPLATE.format(text=text)\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        \"max_tokens\": 500,\n",
    "        \"temperature\": 0.1  # é™ä½éšæœºæ€§ï¼Œæé«˜ä¸€è‡´æ€§\n",
    "    }\n",
    "    \n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            print(f\"ğŸ”„ LLMè°ƒç”¨å°è¯• {attempt + 1}/{max_retries + 1}\")\n",
    "            \n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.post(API_URL, json=payload, headers=headers) as response:\n",
    "                    if response.status != 200:\n",
    "                        print(f\"âŒ APIè°ƒç”¨å¤±è´¥: HTTP {response.status}\")\n",
    "                        if attempt < max_retries:\n",
    "                            await asyncio.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•\n",
    "                            continue\n",
    "                        return None\n",
    "                    \n",
    "                    result = await response.json()\n",
    "                    \n",
    "                    # æå–å›å¤å†…å®¹\n",
    "                    if \"choices\" in result and len(result[\"choices\"]) > 0:\n",
    "                        content = result[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "                        print(f\"ğŸ“ LLMåŸå§‹å›å¤: {content[:200]}...\")\n",
    "                        \n",
    "                        # å°è¯•è§£æJSON\n",
    "                        try:\n",
    "                            # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°\n",
    "                            if content.startswith(\"```json\"):\n",
    "                                content = content[7:]\n",
    "                            if content.endswith(\"```\"):\n",
    "                                content = content[:-3]\n",
    "                            content = content.strip()\n",
    "                            \n",
    "                            # è§£æJSON\n",
    "                            json_data = json.loads(content)\n",
    "                            print(f\"âœ… JSONè§£ææˆåŠŸ\")\n",
    "                            \n",
    "                            # éªŒè¯å¹¶åˆ›å»ºPydanticå¯¹è±¡\n",
    "                            abstract_steps = AbstractSteps(**json_data)\n",
    "                            print(f\"âœ… PydanticéªŒè¯æˆåŠŸ\")\n",
    "                            return abstract_steps\n",
    "                            \n",
    "                        except json.JSONDecodeError as e:\n",
    "                            print(f\"âŒ JSONè§£æå¤±è´¥: {str(e)}\")\n",
    "                            if attempt < max_retries:\n",
    "                                print(f\"ğŸ”„ å‡†å¤‡é‡è¯•...\")\n",
    "                                await asyncio.sleep(1)\n",
    "                                continue\n",
    "                            \n",
    "                        except Exception as e:\n",
    "                            print(f\"âŒ PydanticéªŒè¯å¤±è´¥: {str(e)}\")\n",
    "                            if attempt < max_retries:\n",
    "                                print(f\"ğŸ”„ å‡†å¤‡é‡è¯•...\")\n",
    "                                await asyncio.sleep(1)\n",
    "                                continue\n",
    "                    else:\n",
    "                        print(f\"âŒ APIå“åº”æ ¼å¼å¼‚å¸¸\")\n",
    "                        if attempt < max_retries:\n",
    "                            await asyncio.sleep(1)\n",
    "                            continue\n",
    "                        \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ ç½‘ç»œè¯·æ±‚å¼‚å¸¸: {str(e)}\")\n",
    "            if attempt < max_retries:\n",
    "                await asyncio.sleep(1)\n",
    "                continue\n",
    "    \n",
    "    print(f\"âŒ æ‰€æœ‰é‡è¯•å°è¯•å‡å¤±è´¥\")\n",
    "    return None\n",
    "\n",
    "def extract_text_for_llm(file_path: str) -> tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    ä»markdownæ–‡ä»¶ä¸­æå–ç”¨äºLLMåˆ†æçš„æ–‡æœ¬\n",
    "    \n",
    "    Args:\n",
    "        file_path: markdownæ–‡ä»¶è·¯å¾„\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (æ˜¯å¦æ‰¾åˆ°æ‘˜è¦, åˆ†ææ–‡æœ¬)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # é¦–å…ˆå°è¯•è§„åˆ™æå–æ‘˜è¦\n",
    "        found, abstract_text = extract_abstract_from_md(file_path)\n",
    "        \n",
    "        if found and abstract_text.strip():\n",
    "            print(f\"âœ… è§„åˆ™æå–æˆåŠŸï¼Œä½¿ç”¨æ‘˜è¦æ–‡æœ¬ (é•¿åº¦: {len(abstract_text)} å­—ç¬¦)\")\n",
    "            return True, abstract_text\n",
    "        else:\n",
    "            # è§„åˆ™æå–å¤±è´¥ï¼Œè¯»å–å‰5000å­—ç¬¦\n",
    "            print(f\"âš ï¸ è§„åˆ™æå–å¤±è´¥ï¼Œä½¿ç”¨å‰5000å­—ç¬¦\")\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # å–å‰5000å­—ç¬¦\n",
    "            text_for_llm = content[:5000]\n",
    "            print(f\"ğŸ“„ ä½¿ç”¨å‰5000å­—ç¬¦ (å®é™…é•¿åº¦: {len(text_for_llm)} å­—ç¬¦)\")\n",
    "            return False, text_for_llm\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ–‡ä»¶è¯»å–å¼‚å¸¸: {str(e)}\")\n",
    "        return False, \"\"\n",
    "\n",
    "async def analyze_abstract_steps(file_path: str) -> Optional[AbstractSteps]:\n",
    "    \"\"\"\n",
    "    åˆ†æmarkdownæ–‡ä»¶çš„æ‘˜è¦è¯­æ­¥\n",
    "    \n",
    "    Args:\n",
    "        file_path: markdownæ–‡ä»¶è·¯å¾„\n",
    "    \n",
    "    Returns:\n",
    "        AbstractStepså¯¹è±¡æˆ–None\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ” å¼€å§‹åˆ†ææ–‡ä»¶: {file_path}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 1. æå–åˆ†ææ–‡æœ¬\n",
    "    is_abstract, analysis_text = extract_text_for_llm(file_path)\n",
    "    \n",
    "    if not analysis_text.strip():\n",
    "        print(f\"âŒ æ— æ³•è·å–åˆ†ææ–‡æœ¬\")\n",
    "        return None\n",
    "    \n",
    "    # 2. è°ƒç”¨LLMåˆ†æ\n",
    "    print(f\"\\nğŸ¤– å¼€å§‹LLMè¯­æ­¥åˆ†æ...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    result = await call_deepseek_api(analysis_text)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    elapsed = end_time - start_time\n",
    "    \n",
    "    if result:\n",
    "        print(f\"\\nâœ… åˆ†æå®Œæˆï¼è€—æ—¶: {elapsed:.2f}ç§’\")\n",
    "        print(f\"ğŸ“Š è¯­æ­¥åˆ†æç»“æœ:\")\n",
    "        print(f\"   â€¢ èƒŒæ™¯/é—®é¢˜: {result.background_problem}\")\n",
    "        print(f\"   â€¢ æ–¹æ³•/é€”å¾„: {result.method_approach}\")\n",
    "        print(f\"   â€¢ ç»“æœ: {result.result}\")\n",
    "        print(f\"   â€¢ ç»“è®º/è´¡çŒ®: {result.conclusion_contribution}\")\n",
    "        return result\n",
    "    else:\n",
    "        print(f\"\\nâŒ åˆ†æå¤±è´¥ï¼è€—æ—¶: {elapsed:.2f}ç§’\")\n",
    "        return None\n",
    "\n",
    "# æµ‹è¯•å‡½æ•°\n",
    "async def test_abstract_analysis():\n",
    "    \"\"\"æµ‹è¯•æ‘˜è¦åˆ†æåŠŸèƒ½\"\"\"\n",
    "    print(f\"\\nğŸ§ª å¼€å§‹æµ‹è¯•æ‘˜è¦è¯­æ­¥åˆ†æç³»ç»Ÿ\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # æµ‹è¯•æ–‡ä»¶åˆ—è¡¨\n",
    "    test_files = [\n",
    "        \"/Users/xiaokong/task/2025/paper_vis/vis/md/af705d1369467b0aa55cb59354a84a0e.md\",\n",
    "        \"/Users/xiaokong/task/2025/paper_vis/vis/md/3791465d4e18e4033b5c7bd322c44df2.md\"\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, file_path in enumerate(test_files, 1):\n",
    "        print(f\"\\nğŸ“ æµ‹è¯•æ–‡ä»¶ {i}: {Path(file_path).name}\")\n",
    "        \n",
    "        if not Path(file_path).exists():\n",
    "            print(f\"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\")\n",
    "            continue\n",
    "        \n",
    "        result = await analyze_abstract_steps(file_path)\n",
    "        results.append((file_path, result))\n",
    "        \n",
    "        if i < len(test_files):\n",
    "            print(f\"\\nâ³ ç­‰å¾…2ç§’åå¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶...\")\n",
    "            await asyncio.sleep(2)\n",
    "    \n",
    "    # æ±‡æ€»ç»“æœ\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"ğŸ“ˆ æµ‹è¯•ç»“æœæ±‡æ€»\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    success_count = sum(1 for _, result in results if result is not None)\n",
    "    total_count = len(results)\n",
    "    \n",
    "    print(f\"âœ… æˆåŠŸ: {success_count}/{total_count}\")\n",
    "    print(f\"âŒ å¤±è´¥: {total_count - success_count}/{total_count}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# è¿è¡Œæµ‹è¯•\n",
    "if __name__ == \"__main__\":\n",
    "    # è¿è¡Œå¼‚æ­¥æµ‹è¯•\n",
    "    results = await test_abstract_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379489f4",
   "metadata": {},
   "source": [
    "# æ ‡é¢˜è§„èŒƒåŒ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19611b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1çº§æ ‡é¢˜åˆ—è¡¨: ['# Abstract', '# 1. Introduction', '# 2. Methodology', '# 3. Results', '# Conclusion', '# References']\n",
      "æ ‡é¢˜ç»“æ„åˆ†æ: {'total_headings': 12, 'h1_count': 12, 'h2_count': 0, 'h3_count': 0, 'h4_count': 0, 'h5_count': 0, 'h6_count': 0, 'headings_by_level': {1: ['# Abstract', '# 1. Introduction', '# 1.1 Background', '# 1.1.1 Historical Context', '# 1.2 Related Work', '# 1.2.1 Previous Studies', '# 2. Methodology', '# 2.1 Data Collection', '# 2.1.1 Sampling Strategy', '# 3. Results', '# Conclusion', '# References']}}\n",
      "æ–‡ä»¶ /Users/xiaokong/task/2025/paper_vis/vis/md/3791465d4e18e4033b5c7bd322c44df2.md çš„1çº§æ ‡é¢˜: ['# A cerebrospinal fluid synaptic protein biomarker for prediction of cognitive resilience versus decline in Alzheimerâ€™s disease', '# A list of authors and their affiliations appears at the end of the paper', '# Check for updates', '# Multicohort CSF proteomics for AD biomarker discovery', '# CSF YWHAG:NPTX2 versus established neurodegeneration AD biomarkers', '# CSF YWHAG:NPTX2 in normal aging and ADAD', '# CSF YWHAG:NPTX2 associations with future AD progression', '# Five defined CSF YWHAG:NPTX2 groups for cognitive prognosis', '# Partial plasma proteomic surrogate of CSF YWHAG:NPTX2', '# Discussion', '# Online content', '# References', '# Article', '# Methods', '# Participants', '# Proteomics', '# CI stage classification', '# $\\\\mathbf{A}\\\\mathbf{+}\\\\mathbf{T_{1}}\\\\mathbf{+}$ versus Aâ€“ $\\\\cdot\\\\mathbf{T}_{1}$ âˆ’ classification', '# Statistical analyses', '# Reporting summary', '# Data availability', '# Code availability', '# References', '# Acknowledgements', '# Author contributions', '# Competing interests', '# standardized effect size', '# nature portfolio', '# Statistics', '# Software and code', '# Data', '#', '# Research involving human participants, their data, or biological material', '# PARTICIPANTS', '# BioFINDER2', '# Field-specific reporting', '# Life sciences study design', '# Reporting for specific materials, systems and methods', '# Materials & experimental systems Methods', '# Plants']\n"
     ]
    }
   ],
   "source": [
    "# æ ‡é¢˜å±‚çº§è§„èŒƒåŒ–ç³»ç»Ÿ\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "class HeadingNormalizer:\n",
    "    \"\"\"æ ‡é¢˜å±‚çº§è§„èŒƒåŒ–å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"åˆå§‹åŒ–è§„èŒƒåŒ–å™¨\"\"\"\n",
    "        # å®šä¹‰å·²çŸ¥çš„æ— ç¼–å·ä¸€çº§æ ‡é¢˜å…³é”®è¯\n",
    "        self.unnumbered_h1_keywords = [\n",
    "            'Abstract', 'Introduction', 'Conclusion', 'References', 'Appendix',\n",
    "            'Acknowledgments', 'Acknowledgements', 'Bibliography', 'Index',\n",
    "            'Preface', 'Foreword', 'Summary', 'Executive Summary',\n",
    "            'Table of Contents', 'List of Figures', 'List of Tables',\n",
    "            'Nomenclature', 'Glossary', 'Abbreviations'\n",
    "        ]\n",
    "        \n",
    "        # ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰\n",
    "        self.patterns = [\n",
    "            # ä¸‰çº§æ ‡é¢˜: # 1.1.1. æ ‡é¢˜\n",
    "            (r'^(\\s*)#\\s+(\\d+\\.\\d+\\.\\d+\\.\\s+.*)$', r'\\1### \\2'),\n",
    "            \n",
    "            # äºŒçº§æ ‡é¢˜: # 1.1. æ ‡é¢˜  \n",
    "            (r'^(\\s*)#\\s+(\\d+\\.\\d+\\.\\s+.*)$', r'\\1## \\2'),\n",
    "            \n",
    "            # ä¸€çº§æ ‡é¢˜: # 1. æ ‡é¢˜\n",
    "            (r'^(\\s*)#\\s+(\\d+\\.\\s+.*)$', r'\\1# \\2'),\n",
    "            \n",
    "            # æ— ç¼–å·ä¸€çº§æ ‡é¢˜: # Abstract, # Conclusion ç­‰\n",
    "            (r'^(\\s*)#\\s+(' + '|'.join(self.unnumbered_h1_keywords) + r')(\\s*.*)$', r'\\1# \\2\\3'),\n",
    "        ]\n",
    "    \n",
    "    def normalize_headings(self, markdown_text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        å¯¹å­¦æœ¯è®ºæ–‡çš„Markdownæ–‡æœ¬è¿›è¡Œæ ‡é¢˜å±‚çº§è§„èŒƒåŒ–ï¼Œå¹¶è¿”å›æ‰€æœ‰1çº§æ ‡é¢˜åˆ—è¡¨\n",
    "        \n",
    "        å°†PDFè§£æå·¥å…·é”™è¯¯æ ‡è®°çš„æ‰€æœ‰ä¸€çº§æ ‡é¢˜(#)æ ¹æ®ç¼–å·æ¨¡å¼æ¢å¤ä¸ºæ­£ç¡®çš„æ ‡é¢˜å±‚çº§\n",
    "        \n",
    "        Args:\n",
    "            markdown_text: åŒ…å«è®ºæ–‡å†…å®¹çš„å®Œæ•´Markdownæ–‡æœ¬å­—ç¬¦ä¸²\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: æ‰€æœ‰1çº§æ ‡é¢˜åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        \n",
    "        # æŒ‰è¡Œåˆ†å‰²æ–‡æœ¬\n",
    "        lines = markdown_text.split('\\n')\n",
    "        normalized_lines = []\n",
    "        h1_headings = []  # å­˜å‚¨çœŸæ­£çš„1çº§æ ‡é¢˜\n",
    "        \n",
    "        # ç»Ÿè®¡ä¿¡æ¯\n",
    "        stats = {\n",
    "            'h1_numbered': 0,      # ç¼–å·ä¸€çº§æ ‡é¢˜\n",
    "            'h1_unnumbered': 0,    # æ— ç¼–å·ä¸€çº§æ ‡é¢˜  \n",
    "            'h2': 0,              # äºŒçº§æ ‡é¢˜\n",
    "            'h3': 0,              # ä¸‰çº§æ ‡é¢˜\n",
    "            'unchanged': 0        # æœªåŒ¹é…çš„æ ‡é¢˜\n",
    "        }\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            original_line = line\n",
    "            processed = False\n",
    "            \n",
    "            # æ£€æŸ¥æ˜¯å¦æ˜¯ä»¥#å¼€å¤´çš„æ ‡é¢˜è¡Œ\n",
    "            if re.match(r'^\\s*#\\s+', line):\n",
    "                # æŒ‰ä¼˜å…ˆçº§å°è¯•åŒ¹é…å„ç§æ¨¡å¼\n",
    "                for pattern, replacement in self.patterns:\n",
    "                    if re.match(pattern, line):\n",
    "                        # æ‰§è¡Œæ›¿æ¢\n",
    "                        new_line = re.sub(pattern, replacement, line)\n",
    "                        normalized_lines.append(new_line)\n",
    "                        \n",
    "                        # ç»Ÿè®¡å¤„ç†ç»“æœå¹¶æ”¶é›†1çº§æ ‡é¢˜\n",
    "                        if '###' in new_line:\n",
    "                            stats['h3'] += 1\n",
    "                        elif '##' in new_line:\n",
    "                            stats['h2'] += 1\n",
    "                        elif '# ' in new_line and not re.match(r'^\\s*##', new_line):\n",
    "                            # åªæœ‰çœŸæ­£çš„1çº§æ ‡é¢˜æ‰æ”¶é›†\n",
    "                            if re.match(r'^\\s*#\\s+\\d+\\.', new_line):\n",
    "                                # ç¼–å·çš„ä¸€çº§æ ‡é¢˜ (å¦‚ # 1. Introduction)\n",
    "                                stats['h1_numbered'] += 1\n",
    "                                h1_headings.append(new_line.strip())\n",
    "                            elif self._is_unnumbered_h1(new_line):\n",
    "                                # æ— ç¼–å·çš„ä¸€çº§æ ‡é¢˜ (å¦‚ # Abstract, # Conclusion)\n",
    "                                stats['h1_unnumbered'] += 1\n",
    "                                h1_headings.append(new_line.strip())\n",
    "                        \n",
    "                        processed = True\n",
    "                        break\n",
    "                \n",
    "                # å¦‚æœæ²¡æœ‰åŒ¹é…ä»»ä½•æ¨¡å¼ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºæ— ç¼–å·çš„1çº§æ ‡é¢˜\n",
    "                if not processed:\n",
    "                    normalized_lines.append(line)\n",
    "                    if self._is_unnumbered_h1(line):\n",
    "                        stats['h1_unnumbered'] += 1\n",
    "                        h1_headings.append(line.strip())\n",
    "                    else:\n",
    "                        stats['unchanged'] += 1\n",
    "            else:\n",
    "                # éæ ‡é¢˜è¡Œï¼Œç›´æ¥æ·»åŠ \n",
    "                normalized_lines.append(line)\n",
    "        \n",
    "        return h1_headings\n",
    "    \n",
    "    def _is_unnumbered_h1(self, line: str) -> bool:\n",
    "        \"\"\"\n",
    "        åˆ¤æ–­æ˜¯å¦ä¸º1çº§æ ‡é¢˜\n",
    "        \n",
    "        è§„åˆ™ï¼š\n",
    "        1. æ‰€æœ‰ä»¥#å¼€å¤´çš„éƒ½æ˜¯æ ‡é¢˜\n",
    "        2. é€šè¿‡åºå·æ ¼å¼åˆ¤æ–­çº§åˆ«ï¼š\n",
    "           - 1, 1. â†’ 1çº§æ ‡é¢˜\n",
    "           - 1.1, 1.1. â†’ 2çº§æ ‡é¢˜\n",
    "           - 1.1.1, 1.1.1. â†’ 3çº§æ ‡é¢˜\n",
    "        3. æ²¡æœ‰åºå·çš„æ ‡é¢˜é»˜è®¤ä¸º1çº§æ ‡é¢˜\n",
    "        \n",
    "        Args:\n",
    "            line: æ ‡é¢˜è¡Œ\n",
    "        \n",
    "        Returns:\n",
    "            bool: æ˜¯å¦ä¸º1çº§æ ‡é¢˜\n",
    "        \"\"\"\n",
    "        # æå–æ ‡é¢˜æ–‡æœ¬ï¼ˆå»æ‰#å’Œç©ºæ ¼ï¼‰\n",
    "        title_text = re.sub(r'^\\s*#+\\s*', '', line).strip()\n",
    "        \n",
    "        # 1. æ£€æŸ¥æ˜¯å¦åŒ¹é…å·²çŸ¥çš„æ— ç¼–å·1çº§æ ‡é¢˜å…³é”®è¯ï¼ˆç²¾ç¡®åŒ¹é…ï¼‰\n",
    "        for keyword in self.unnumbered_h1_keywords:\n",
    "            if title_text.lower() == keyword.lower():\n",
    "                return True\n",
    "        \n",
    "        # 2. æ£€æŸ¥åºå·æ ¼å¼æ¥åˆ¤æ–­æ ‡é¢˜çº§åˆ«\n",
    "        # 2.1 åŒ¹é…1çº§æ ‡é¢˜æ ¼å¼ï¼šçº¯æ•°å­—å¼€å¤´ï¼ˆå¦‚ \"1\", \"1.\", \"2\", \"2.\"ï¼‰\n",
    "        if re.match(r'^\\d+\\.?\\s+', title_text):\n",
    "            # æ£€æŸ¥æ˜¯å¦åŒ…å«å°æ•°ç‚¹ï¼ˆå¦‚ 1.1, 1.1.1ï¼‰\n",
    "            if not re.search(r'\\d+\\.\\d+', title_text):\n",
    "                return True  # è¿™æ˜¯1çº§æ ‡é¢˜\n",
    "        \n",
    "        # 2.2 æ£€æŸ¥æ˜¯å¦ä¸º2çº§æˆ–3çº§æ ‡é¢˜æ ¼å¼\n",
    "        if re.search(r'\\d+\\.\\d+', title_text):\n",
    "            return False  # è¿™æ˜¯2çº§æˆ–3çº§æ ‡é¢˜\n",
    "        \n",
    "        # 3. æ²¡æœ‰åºå·çš„æ ‡é¢˜é»˜è®¤ä¸º1çº§æ ‡é¢˜\n",
    "        return True\n",
    "\n",
    "\n",
    "    def extract_headings_only(self, markdown_text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        æå–è§„èŒƒåŒ–åçš„æ‰€æœ‰æ ‡é¢˜è¡Œ\n",
    "        \n",
    "        Args:\n",
    "            markdown_text: Markdownæ–‡æœ¬\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: æ‰€æœ‰æ ‡é¢˜è¡Œçš„åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        lines = markdown_text.split('\\n')\n",
    "        headings = []\n",
    "        \n",
    "        for line in lines:\n",
    "            if re.match(r'^\\s*#+\\s+', line):\n",
    "                headings.append(line.strip())\n",
    "        \n",
    "        return headings\n",
    "    \n",
    "    def extract_h1_headings(self, markdown_text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        æå–æ‰€æœ‰1çº§æ ‡é¢˜ï¼ŒæŒ‰åœ¨æ–‡æ¡£ä¸­å‡ºç°çš„é¡ºåºè¿”å›\n",
    "        \n",
    "        Args:\n",
    "            markdown_text: Markdownæ–‡æœ¬\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: 1çº§æ ‡é¢˜è¡Œçš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯å®Œæ•´çš„åŸå§‹è¡Œå­—ç¬¦ä¸²ï¼ˆåŒ…å«#å·ï¼‰\n",
    "        \"\"\"\n",
    "        lines = markdown_text.split('\\n')\n",
    "        h1_headings = []\n",
    "        \n",
    "        for line in lines:\n",
    "            line = line.strip()\n",
    "            # æ£€æŸ¥æ˜¯å¦ä¸ºæ ‡é¢˜è¡Œ\n",
    "            if line.startswith('#'):\n",
    "                # æ£€æŸ¥æ˜¯å¦ä¸º1çº§æ ‡é¢˜\n",
    "                if self._is_unnumbered_h1(line):\n",
    "                    h1_headings.append(line)  # è¿”å›å®Œæ•´çš„åŸå§‹è¡Œï¼ˆåŒ…å«#å·ï¼‰\n",
    "        \n",
    "        return h1_headings\n",
    "\n",
    "    def analyze_heading_structure(self, markdown_text: str) -> dict:\n",
    "        \"\"\"\n",
    "        åˆ†ææ ‡é¢˜ç»“æ„ï¼Œè¿”å›å±‚çº§ç»Ÿè®¡ä¿¡æ¯\n",
    "        \n",
    "        Args:\n",
    "            markdown_text: Markdownæ–‡æœ¬\n",
    "        \n",
    "        Returns:\n",
    "            dict: æ ‡é¢˜ç»“æ„åˆ†æç»“æœ\n",
    "        \"\"\"\n",
    "        headings = self.extract_headings_only(markdown_text)\n",
    "        \n",
    "        structure = {\n",
    "            'total_headings': len(headings),\n",
    "            'h1_count': 0,\n",
    "            'h2_count': 0, \n",
    "            'h3_count': 0,\n",
    "            'h4_count': 0,\n",
    "            'h5_count': 0,\n",
    "            'h6_count': 0,\n",
    "            'headings_by_level': {}\n",
    "        }\n",
    "        \n",
    "        for heading in headings:\n",
    "            # è®¡ç®—æ ‡é¢˜å±‚çº§\n",
    "            level = len(re.match(r'^#+', heading).group())\n",
    "            structure[f'h{level}_count'] += 1\n",
    "            \n",
    "            if level not in structure['headings_by_level']:\n",
    "                structure['headings_by_level'][level] = []\n",
    "            structure['headings_by_level'][level].append(heading)\n",
    "        \n",
    "        return structure\n",
    "\n",
    "    def process_markdown_file(self, file_path: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        å¤„ç†å®é™…çš„Markdownæ–‡ä»¶\n",
    "        \n",
    "        Args:\n",
    "            file_path: Markdownæ–‡ä»¶è·¯å¾„\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: æ‰€æœ‰1çº§æ ‡é¢˜åˆ—è¡¨\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # è¯»å–æ–‡ä»¶\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # è§„èŒƒåŒ–å¤„ç†ï¼Œè¿”å›1çº§æ ‡é¢˜åˆ—è¡¨\n",
    "            h1_headings = self.normalize_headings(content)\n",
    "            \n",
    "            return h1_headings\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            return []\n",
    "\n",
    "# è¿è¡Œæµ‹è¯•\n",
    "if __name__ == \"__main__\":\n",
    "    # åˆ›å»ºè§„èŒƒåŒ–å™¨å®ä¾‹\n",
    "    normalizer = HeadingNormalizer()\n",
    "    \n",
    "    # æµ‹è¯•ç”¨ä¾‹1: æ¨¡æ‹ŸPDFè§£æåçš„é”™è¯¯æ ‡é¢˜æ ¼å¼\n",
    "    test_text_1 = \"\"\"# Abstract\n",
    "This is the abstract content.\n",
    "\n",
    "# 1. Introduction\n",
    "This is the introduction.\n",
    "\n",
    "# 1.1 Background\n",
    "Background information here.\n",
    "\n",
    "# 1.1.1 Historical Context\n",
    "Historical context details.\n",
    "\n",
    "# 1.2 Related Work\n",
    "Related work section.\n",
    "\n",
    "# 1.2.1 Previous Studies\n",
    "Previous studies details.\n",
    "\n",
    "# 2. Methodology\n",
    "Methodology section.\n",
    "\n",
    "# 2.1 Data Collection\n",
    "Data collection methods.\n",
    "\n",
    "# 2.1.1 Sampling Strategy\n",
    "Sampling strategy details.\n",
    "\n",
    "# 3. Results\n",
    "Results section.\n",
    "\n",
    "# Conclusion\n",
    "This is the conclusion.\n",
    "\n",
    "# References\n",
    "References list.\"\"\"\n",
    "    \n",
    "    # è§„èŒƒåŒ–å¤„ç†ï¼Œè¿”å›1çº§æ ‡é¢˜åˆ—è¡¨\n",
    "    h1_headings = normalizer.normalize_headings(test_text_1)\n",
    "    print(\"1çº§æ ‡é¢˜åˆ—è¡¨:\", h1_headings)\n",
    "    \n",
    "    # åˆ†ææ ‡é¢˜ç»“æ„\n",
    "    structure = normalizer.analyze_heading_structure(test_text_1)\n",
    "    print(\"æ ‡é¢˜ç»“æ„åˆ†æ:\", structure)\n",
    "    \n",
    "    # å¯é€‰ï¼šå¤„ç†å®é™…æ–‡ä»¶\n",
    "    test_files = [\n",
    "         \"/Users/xiaokong/task/2025/paper_vis/vis/md/3791465d4e18e4033b5c7bd322c44df2.md\",\n",
    "    ]\n",
    "    \n",
    "    for file_path in test_files:\n",
    "        if Path(file_path).exists():\n",
    "            h1_headings = normalizer.process_markdown_file(file_path)\n",
    "            print(f\"æ–‡ä»¶ {file_path} çš„1çº§æ ‡é¢˜:\", h1_headings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b340660a",
   "metadata": {},
   "source": [
    "# LLMæ ‡é¢˜æ˜ å°„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a8a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "æ ‡é¢˜æ˜ å°„LLMæ¨¡å—\n",
    "åˆ©ç”¨LLMå¯¹è®ºæ–‡ç« èŠ‚æ ‡é¢˜è¿›è¡Œåˆ†ç±»æ˜ å°„åˆ°å››ä¸ªæ ‡å‡†æ³³é“\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import logging\n",
    "from typing import List, Dict, Optional, Any\n",
    "import time\n",
    "\n",
    "class TitleMappingLLM:\n",
    "    \"\"\"æ ‡é¢˜æ˜ å°„LLMå¤„ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self, api_url: str = None, api_key: str = None, model: str = None):\n",
    "        \"\"\"\n",
    "        åˆå§‹åŒ–LLMå¤„ç†å™¨\n",
    "        \n",
    "        Args:\n",
    "            api_url: LLM APIåœ°å€\n",
    "            api_key: APIå¯†é’¥\n",
    "            model: ä½¿ç”¨çš„æ¨¡å‹åç§°\n",
    "        \"\"\"\n",
    "        # å†™æ­»çš„LLMé…ç½®\n",
    "        self.api_url = api_url or \"https://api.deepseek.com/v1/chat/completions\"\n",
    "        self.api_key = api_key or \"sk-8de35978ccec41e39e2b9ebfc90b7aa1\"\n",
    "        self.model = model or \"deepseek-chat\"\n",
    "        \n",
    "        # è®¾ç½®æ—¥å¿—\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # ç³»ç»Ÿæç¤ºè¯\n",
    "        self.system_prompt = \"\"\"You are a top-tier **Academic Paper Structure Analyst** specializing in **cross-disciplinary semantic filtering and classification**. Your task is to accurately map chapter titles, which represent the **core research logic flow**, from a provided list of titles into four standard swimlanes.\n",
    "\n",
    "**[Core Filtering and Mapping Rules]**\n",
    "1.  **Filtering (Noise Reduction):** You must **ignore and discard** the following types of titles:\n",
    "    * **Non-chapter content:** Paper main titles, author lists, publication metadata (e.g., \"Article\", \"Online content\", \"Check for updates\", \"Reporting summary\", \"Data availability\", \"Author contributions\", \"Competing interests\", etc.).\n",
    "    * **Boundary anchors:** \"Abstract\" (or its variants), \"References\", \"Acknowledgements\", \"Appendix\".\n",
    "2.  **Lane Assignment (Classification):** Only assign the filtered **valid core chapters** to the following **Four Standard Swimlanes**.\n",
    "3.  **Quota Constraint (Max: 2):** The number of titles assigned to each standard swimlane **must not exceed two (Max: 2)**. If multiple titles belong to the same swimlane, you must select the core titles that best represent the function of that swimlane.\n",
    "\n",
    "**[Four Standard Swimlanes]**\n",
    "1.  Context & Related Work\n",
    "2.  Methodology & Setup\n",
    "3.  Results & Analysis\n",
    "4.  Conclusion\n",
    "\n",
    "**[CRITICAL FORMATTING REQUIREMENTS]**\n",
    "1.  **Strictly and uniquely** output a JSON object conforming to the JSON structure.\n",
    "2.  The Key must be the **Standard Swimlane Name**, and the Value must be an **array** containing the **EXACT original title strings**.\n",
    "3.  **PRESERVE EXACT FORMAT:** You MUST preserve the exact original format of titles including ALL symbols, numbers, punctuation, capitalization, and spacing (e.g., \"# 1. Introduction\", \"# 2. Related Work\", etc.).\n",
    "4.  **Absolutely forbid** outputting any explanations, preambles, summaries, or extra text.\"\"\"\n",
    "\n",
    "        # ç”¨æˆ·æç¤ºè¯æ¨¡æ¿\n",
    "        self.user_prompt_template = \"\"\"Please analyze the **raw title list** provided below, which originates from a paper parser. Strictly adhere to the **filtering and quota constraints** rules specified in the system instructions to classify and map the core chapter titles into the four standard swimlanes.\n",
    "\n",
    "**[Title List to be Processed]**\n",
    "{title_list}\n",
    "\n",
    "**[Example of Desired JSON Structure]**\n",
    "Please strictly output your results according to the following concise structure, where the **Key is the Swimlane Name and the Value is an array of EXACT original title strings** (preserving all formatting including \"#\", numbers, punctuation):\n",
    "\n",
    "{{\n",
    "  \"Context & Related Work\": [\"# 1. Introduction\", \"# 2. Related Work\"],\n",
    "  \"Methodology & Setup\": [\"# 3. Methodology\"],\n",
    "  \"Results & Analysis\": [\"# 4. Results\", \"# 5. Discussion\"],\n",
    "  \"Conclusion\": [\"# 6. Conclusion\"]\n",
    "}}\"\"\"\n",
    "\n",
    "    def _call_llm_api(self, messages: List[Dict[str, str]], max_retries: int = 3) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        è°ƒç”¨LLM API\n",
    "        \n",
    "        Args:\n",
    "            messages: æ¶ˆæ¯åˆ—è¡¨\n",
    "            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°\n",
    "            \n",
    "        Returns:\n",
    "            LLMå“åº”å†…å®¹\n",
    "        \"\"\"\n",
    "        headers = {\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\"\n",
    "        }\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": 0.1,  # ä½æ¸©åº¦ç¡®ä¿ä¸€è‡´æ€§\n",
    "            \"max_tokens\": 1000\n",
    "        }\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    self.api_url,\n",
    "                    headers=headers,\n",
    "                    json=payload,\n",
    "                    timeout=30\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    result = response.json()\n",
    "                    return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "                else:\n",
    "                    self.logger.warning(f\"APIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}\")\n",
    "                    \n",
    "            except requests.exceptions.RequestException as e:\n",
    "                self.logger.warning(f\"APIè°ƒç”¨å¼‚å¸¸ (å°è¯• {attempt + 1}/{max_retries}): {e}\")\n",
    "                \n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿\n",
    "                \n",
    "        self.logger.error(\"LLM APIè°ƒç”¨å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°\")\n",
    "        return None\n",
    "\n",
    "    def _parse_json_response(self, response: str) -> Optional[Dict[str, List[str]]]:\n",
    "        \"\"\"\n",
    "        è§£æLLMè¿”å›çš„JSONå“åº”\n",
    "        \n",
    "        Args:\n",
    "            response: LLMå“åº”å­—ç¬¦ä¸²\n",
    "            \n",
    "        Returns:\n",
    "            è§£æåçš„å­—å…¸ï¼Œå¦‚æœè§£æå¤±è´¥è¿”å›None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # å°è¯•ç›´æ¥è§£æ\n",
    "            result = json.loads(response)\n",
    "            \n",
    "            # éªŒè¯ç»“æ„\n",
    "            expected_keys = {\n",
    "                \"Context & Related Work\",\n",
    "                \"Methodology & Setup\", \n",
    "                \"Results & Analysis\",\n",
    "                \"Conclusion\"\n",
    "            }\n",
    "            \n",
    "            if not all(key in result for key in expected_keys):\n",
    "                self.logger.warning(\"JSONç»“æ„ä¸å®Œæ•´ï¼Œç¼ºå°‘å¿…è¦çš„é”®\")\n",
    "                return None\n",
    "                \n",
    "            # éªŒè¯å€¼ç±»å‹\n",
    "            for key, value in result.items():\n",
    "                if not isinstance(value, list):\n",
    "                    self.logger.warning(f\"é”® '{key}' çš„å€¼ä¸æ˜¯åˆ—è¡¨ç±»å‹\")\n",
    "                    return None\n",
    "                    \n",
    "            return result\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            self.logger.warning(f\"JSONè§£æå¤±è´¥: {e}\")\n",
    "            \n",
    "            # å°è¯•æå–JSONéƒ¨åˆ†\n",
    "            try:\n",
    "                # æŸ¥æ‰¾JSONå¼€å§‹å’Œç»“æŸä½ç½®\n",
    "                start_idx = response.find('{')\n",
    "                end_idx = response.rfind('}')\n",
    "                \n",
    "                if start_idx != -1 and end_idx != -1 and end_idx > start_idx:\n",
    "                    json_str = response[start_idx:end_idx + 1]\n",
    "                    result = json.loads(json_str)\n",
    "                    return result\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "                \n",
    "            return None\n",
    "\n",
    "    def map_titles(self, title_list: List[str]) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        å°†æ ‡é¢˜åˆ—è¡¨æ˜ å°„åˆ°å››ä¸ªæ ‡å‡†æ³³é“\n",
    "        \n",
    "        Args:\n",
    "            title_list: åŸå§‹æ ‡é¢˜åˆ—è¡¨\n",
    "            \n",
    "        Returns:\n",
    "            å¹²å‡€çš„æ˜ å°„ç»“æœå­—å…¸\n",
    "        \"\"\"\n",
    "        if not title_list:\n",
    "            self.logger.error(\"æ ‡é¢˜åˆ—è¡¨ä¸ºç©º\")\n",
    "            return {}\n",
    "            \n",
    "        if not self.api_key:\n",
    "            self.logger.error(\"APIå¯†é’¥æœªè®¾ç½®\")\n",
    "            return {}\n",
    "        \n",
    "        try:\n",
    "            # æ„å»ºç”¨æˆ·æç¤ºè¯\n",
    "            title_list_str = \"\\n\".join([f\"'{title}'\" for title in title_list])\n",
    "            user_prompt = self.user_prompt_template.format(title_list=title_list_str)\n",
    "            \n",
    "            # æ„å»ºæ¶ˆæ¯\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "            \n",
    "            self.logger.info(f\"å¼€å§‹å¤„ç† {len(title_list)} ä¸ªæ ‡é¢˜\")\n",
    "            \n",
    "            # è°ƒç”¨LLM API\n",
    "            response = self._call_llm_api(messages)\n",
    "            \n",
    "            if response is None:\n",
    "                self.logger.error(\"LLM APIè°ƒç”¨å¤±è´¥\")\n",
    "                return {}\n",
    "            \n",
    "            # è§£æå“åº”\n",
    "            result = self._parse_json_response(response)\n",
    "            \n",
    "            if result is None:\n",
    "                self.logger.error(f\"LLMå“åº”è§£æå¤±è´¥ï¼ŒåŸå§‹å“åº”: {response}\")\n",
    "                return {}\n",
    "            \n",
    "            # éªŒè¯ç»“æœ\n",
    "            total_mapped = sum(len(titles) for titles in result.values())\n",
    "            self.logger.info(f\"æˆåŠŸæ˜ å°„ {total_mapped} ä¸ªæ ‡é¢˜åˆ°å››ä¸ªæ³³é“\")\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"æ ‡é¢˜æ˜ å°„è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def map_titles_with_debug(self, title_list: List[str]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        å¸¦è°ƒè¯•ä¿¡æ¯çš„æ ‡é¢˜æ˜ å°„\n",
    "        \n",
    "        Args:\n",
    "            title_list: åŸå§‹æ ‡é¢˜åˆ—è¡¨\n",
    "            \n",
    "        Returns:\n",
    "            åŒ…å«è¯¦ç»†è°ƒè¯•ä¿¡æ¯çš„æ˜ å°„ç»“æœ\n",
    "        \"\"\"\n",
    "        debug_info = {\n",
    "            \"input_titles\": title_list,\n",
    "            \"input_count\": len(title_list),\n",
    "            \"api_url\": self.api_url,\n",
    "            \"model\": self.model,\n",
    "            \"timestamp\": time.time()\n",
    "        }\n",
    "        \n",
    "        result = self.map_titles(title_list)\n",
    "        result[\"debug_info\"] = debug_info\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"æµ‹è¯•å‡½æ•°\"\"\"\n",
    "    # ç¤ºä¾‹ä½¿ç”¨\n",
    "    title_list = [\n",
    "        \"# A Q-learning approach to the continuous control problem of robot inverted pendulum balancing\",\n",
    "        \"# Corresponding Author:\",\n",
    "        \"# Abstract\",\n",
    "        \"# 1. Introduction\",\n",
    "        \"# 2. Proposed approach and background\", \n",
    "        \"# 3. Methodologies\",\n",
    "        \"# 4. Results and discussion\",\n",
    "        \"# 5. Conclusion\",\n",
    "        \"# Acknowledgements\",\n",
    "        \"# References\"\n",
    "    ]\n",
    "    \n",
    "    # åˆå§‹åŒ–å¤„ç†å™¨ï¼ˆéœ€è¦è®¾ç½®APIå¯†é’¥ï¼‰\n",
    "    mapper = TitleMappingLLM(\n",
    "        api_key=\"your-api-key-here\",  # æ›¿æ¢ä¸ºå®é™…çš„APIå¯†é’¥\n",
    "        model=\"gpt-3.5-turbo\"\n",
    "    )\n",
    "    \n",
    "    # æ‰§è¡Œæ˜ å°„\n",
    "    result = mapper.map_titles_with_debug(title_list)\n",
    "    \n",
    "    print(\"=== æ ‡é¢˜æ˜ å°„ç»“æœ ===\")\n",
    "    print(f\"æˆåŠŸ: {result['success']}\")\n",
    "    \n",
    "    if result['success']:\n",
    "        print(\"æ˜ å°„ç»“æœ:\")\n",
    "        print(json.dumps(result['result'], indent=2, ensure_ascii=False))\n",
    "        print(\"\\nç»Ÿè®¡ä¿¡æ¯:\")\n",
    "        print(json.dumps(result['statistics'], indent=2, ensure_ascii=False))\n",
    "    else:\n",
    "        print(f\"é”™è¯¯: {result['error']}\")\n",
    "        if 'raw_response' in result:\n",
    "            print(f\"åŸå§‹å“åº”: {result['raw_response']}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75dab40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f95e5e07",
   "metadata": {},
   "source": [
    "# æ ‡é¢˜å†…å®¹åˆ†å‰²-æŠ½å–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e012416e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "æ ¹æ®æ ‡é¢˜æå–Markdownæ–‡ä»¶å†…å®¹çš„å·¥å…·\n",
    "\n",
    "åŠŸèƒ½ï¼š\n",
    "- æ ¹æ®æ ‡é¢˜åˆ—è¡¨å’ŒæŒ‡å®šæ ‡é¢˜ï¼Œæå–è¯¥æ ‡é¢˜åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜ä¹‹é—´çš„å†…å®¹\n",
    "- æ”¯æŒå¤„ç†æœ€åä¸€ä¸ªæ ‡é¢˜çš„æƒ…å†µï¼ˆæå–åˆ°æ–‡ä»¶ç»“æŸï¼‰\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class ContentExtractor:\n",
    "    \"\"\"æ ¹æ®æ ‡é¢˜æå–Markdownæ–‡ä»¶å†…å®¹çš„ç±»\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"åˆå§‹åŒ–å†…å®¹æå–å™¨\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _normalizeHeading(self, heading: str) -> str:\n",
    "        \"\"\"\n",
    "        æ ‡å‡†åŒ–æ ‡é¢˜æ ¼å¼ï¼Œç”¨äºçµæ´»åŒ¹é…\n",
    "        \n",
    "        Args:\n",
    "            heading: åŸå§‹æ ‡é¢˜\n",
    "        \n",
    "        Returns:\n",
    "            str: æ ‡å‡†åŒ–åçš„æ ‡é¢˜\n",
    "        \"\"\"\n",
    "        if not heading:\n",
    "            return \"\"\n",
    "        \n",
    "        # å»é™¤é¦–å°¾ç©ºæ ¼\n",
    "        normalized = heading.strip()\n",
    "        \n",
    "        # å¦‚æœæ ‡é¢˜ä»¥#å¼€å¤´ï¼Œä¿ç•™#å’Œåé¢çš„å†…å®¹\n",
    "        if normalized.startswith('#'):\n",
    "            # æå–#åé¢çš„å†…å®¹\n",
    "            content = normalized[1:].strip()\n",
    "            return f\"#{content}\"\n",
    "        else:\n",
    "            # å¦‚æœæ²¡æœ‰#ï¼Œç›´æ¥è¿”å›å»é™¤ç©ºæ ¼åçš„å†…å®¹\n",
    "            return normalized\n",
    "    \n",
    "    def _extractHeadingContent(self, heading: str) -> str:\n",
    "        \"\"\"\n",
    "        æå–æ ‡é¢˜çš„æ ¸å¿ƒå†…å®¹ï¼ˆå»é™¤#ã€åºå·ç­‰ï¼‰\n",
    "        \n",
    "        Args:\n",
    "            heading: æ ‡é¢˜\n",
    "        \n",
    "        Returns:\n",
    "            str: æ ¸å¿ƒå†…å®¹\n",
    "        \"\"\"\n",
    "        if not heading:\n",
    "            return \"\"\n",
    "        \n",
    "        # å»é™¤é¦–å°¾ç©ºæ ¼\n",
    "        content = heading.strip()\n",
    "        \n",
    "        # å»é™¤å¼€å¤´çš„#å·\n",
    "        if content.startswith('#'):\n",
    "            content = content[1:].strip()\n",
    "        \n",
    "        # å»é™¤å¼€å¤´çš„åºå·ï¼ˆå¦‚ \"1. \"ã€\"2. \"ç­‰ï¼‰\n",
    "        # åŒ¹é…æ¨¡å¼ï¼šæ•°å­— + ç‚¹ + ç©ºæ ¼\n",
    "        content = re.sub(r'^\\d+\\.\\s*', '', content)\n",
    "        \n",
    "        # å»é™¤å¤šä½™ç©ºæ ¼\n",
    "        content = re.sub(r'\\s+', ' ', content).strip()\n",
    "        \n",
    "        return content\n",
    "    \n",
    "    def _isHeadingMatch(self, heading1: str, heading2: str) -> bool:\n",
    "        \"\"\"\n",
    "        åˆ¤æ–­ä¸¤ä¸ªæ ‡é¢˜æ˜¯å¦åŒ¹é…ï¼ˆæ”¯æŒçµæ´»åŒ¹é…ï¼‰\n",
    "        \n",
    "        Args:\n",
    "            heading1: æ ‡é¢˜1\n",
    "            heading2: æ ‡é¢˜2\n",
    "        \n",
    "        Returns:\n",
    "            bool: æ˜¯å¦åŒ¹é…\n",
    "        \"\"\"\n",
    "        # æ ‡å‡†åŒ–ä¸¤ä¸ªæ ‡é¢˜\n",
    "        norm1 = self._normalizeHeading(heading1)\n",
    "        norm2 = self._normalizeHeading(heading2)\n",
    "        \n",
    "        # æå–æ ¸å¿ƒå†…å®¹\n",
    "        content1 = self._extractHeadingContent(norm1)\n",
    "        content2 = self._extractHeadingContent(norm2)\n",
    "        \n",
    "        # å®Œå…¨åŒ¹é…\n",
    "        if content1 == content2:\n",
    "            return True\n",
    "        \n",
    "        # å¿½ç•¥å¤§å°å†™åŒ¹é…\n",
    "        if content1.lower() == content2.lower():\n",
    "            return True\n",
    "        \n",
    "        # å»é™¤æ‰€æœ‰ç©ºæ ¼ååŒ¹é…\n",
    "        clean1 = re.sub(r'\\s+', '', content1)\n",
    "        clean2 = re.sub(r'\\s+', '', content2)\n",
    "        if clean1.lower() == clean2.lower():\n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def extractContentByHeading(self, headingList: List[str], filePath: str, targetHeading: str) -> str:\n",
    "        \"\"\"\n",
    "        æ ¹æ®æ ‡é¢˜åˆ—è¡¨å’ŒæŒ‡å®šæ ‡é¢˜æå–å†…å®¹\n",
    "        \n",
    "        Args:\n",
    "            headingList: æ ‡é¢˜åˆ—è¡¨ï¼Œä¾‹å¦‚ ['# Abstract', '# 1. Introduction', '# 2. Methods']\n",
    "            filePath: Markdownæ–‡ä»¶è·¯å¾„\n",
    "            targetHeading: ç›®æ ‡æ ‡é¢˜ï¼Œä¾‹å¦‚ \"# 5. Conclusion\"\n",
    "        \n",
    "        Returns:\n",
    "            str: æå–çš„å†…å®¹æ–‡æœ¬\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # è¯»å–æ–‡ä»¶å†…å®¹\n",
    "            with open(filePath, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "            \n",
    "            # æŒ‰è¡Œåˆ†å‰²å†…å®¹\n",
    "            lines = content.split('\\n')\n",
    "            \n",
    "            # æ‰¾åˆ°ç›®æ ‡æ ‡é¢˜åœ¨æ ‡é¢˜åˆ—è¡¨ä¸­çš„ä½ç½®\n",
    "            targetIndex = self._findHeadingIndex(headingList, targetHeading)\n",
    "            if targetIndex == -1:\n",
    "                return f\"é”™è¯¯ï¼šåœ¨æ ‡é¢˜åˆ—è¡¨ä¸­æœªæ‰¾åˆ°ç›®æ ‡æ ‡é¢˜ '{targetHeading}'\"\n",
    "            \n",
    "            # æ‰¾åˆ°ç›®æ ‡æ ‡é¢˜åœ¨æ–‡ä»¶ä¸­çš„ä½ç½®\n",
    "            targetLineIndex = self._findHeadingInFile(lines, targetHeading)\n",
    "            if targetLineIndex == -1:\n",
    "                return f\"é”™è¯¯ï¼šåœ¨æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç›®æ ‡æ ‡é¢˜ '{targetHeading}'\"\n",
    "            \n",
    "            # ç¡®å®šç»“æŸä½ç½®\n",
    "            if targetIndex == len(headingList) - 1:\n",
    "                # å¦‚æœæ˜¯æœ€åä¸€ä¸ªæ ‡é¢˜ï¼Œæå–åˆ°æ–‡ä»¶ç»“æŸ\n",
    "                endLineIndex = len(lines)\n",
    "            else:\n",
    "                # æ‰¾åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜åœ¨æ–‡ä»¶ä¸­çš„ä½ç½®\n",
    "                nextHeading = headingList[targetIndex + 1]\n",
    "                endLineIndex = self._findHeadingInFile(lines, nextHeading)\n",
    "                if endLineIndex == -1:\n",
    "                    return f\"é”™è¯¯ï¼šåœ¨æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜ '{nextHeading}'\"\n",
    "            \n",
    "            # æå–å†…å®¹\n",
    "            extractedLines = lines[targetLineIndex:endLineIndex]\n",
    "            extractedContent = '\\n'.join(extractedLines)\n",
    "            \n",
    "            return extractedContent.strip()\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            return f\"é”™è¯¯ï¼šæ–‡ä»¶ '{filePath}' ä¸å­˜åœ¨\"\n",
    "        except Exception as e:\n",
    "            return f\"é”™è¯¯ï¼šå¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸ - {str(e)}\"\n",
    "    \n",
    "    def _findHeadingIndex(self, headingList: List[str], targetHeading: str) -> int:\n",
    "        \"\"\"\n",
    "        åœ¨æ ‡é¢˜åˆ—è¡¨ä¸­æ‰¾åˆ°ç›®æ ‡æ ‡é¢˜çš„ç´¢å¼•ï¼ˆæ”¯æŒçµæ´»åŒ¹é…ï¼‰\n",
    "        \n",
    "        Args:\n",
    "            headingList: æ ‡é¢˜åˆ—è¡¨\n",
    "            targetHeading: ç›®æ ‡æ ‡é¢˜\n",
    "        \n",
    "        Returns:\n",
    "            int: æ ‡é¢˜ç´¢å¼•ï¼Œæœªæ‰¾åˆ°è¿”å›-1\n",
    "        \"\"\"\n",
    "        for i, heading in enumerate(headingList):\n",
    "            if self._isHeadingMatch(heading, targetHeading):\n",
    "                return i\n",
    "        return -1\n",
    "    \n",
    "    def _findHeadingInFile(self, lines: List[str], heading: str) -> int:\n",
    "        \"\"\"\n",
    "        åœ¨æ–‡ä»¶è¡Œä¸­æ‰¾åˆ°æ ‡é¢˜çš„ä½ç½®ï¼ˆæ”¯æŒçµæ´»åŒ¹é…ï¼‰\n",
    "        \n",
    "        Args:\n",
    "            lines: æ–‡ä»¶è¡Œåˆ—è¡¨\n",
    "            heading: è¦æŸ¥æ‰¾çš„æ ‡é¢˜\n",
    "        \n",
    "        Returns:\n",
    "            int: è¡Œç´¢å¼•ï¼Œæœªæ‰¾åˆ°è¿”å›-1\n",
    "        \"\"\"\n",
    "        for i, line in enumerate(lines):\n",
    "            if self._isHeadingMatch(line, heading):\n",
    "                return i\n",
    "        return -1\n",
    "    \n",
    "    def extractContentByHeadingWithDebug(self, headingList: List[str], filePath: str, targetHeading: str) -> dict:\n",
    "        \"\"\"\n",
    "        å¸¦è°ƒè¯•ä¿¡æ¯çš„æå–å†…å®¹æ–¹æ³•\n",
    "        \n",
    "        Args:\n",
    "            headingList: æ ‡é¢˜åˆ—è¡¨\n",
    "            filePath: Markdownæ–‡ä»¶è·¯å¾„\n",
    "            targetHeading: ç›®æ ‡æ ‡é¢˜\n",
    "        \n",
    "        Returns:\n",
    "            dict: åŒ…å«ç»“æœå’Œè°ƒè¯•ä¿¡æ¯çš„å­—å…¸\n",
    "        \"\"\"\n",
    "        result = {\n",
    "            'success': False,\n",
    "            'content': '',\n",
    "            'debug_info': {\n",
    "                'target_heading': targetHeading,\n",
    "                'target_index': -1,\n",
    "                'next_heading': '',\n",
    "                'start_line': -1,\n",
    "                'end_line': -1,\n",
    "                'total_lines': 0,\n",
    "                'error': ''\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # è¯»å–æ–‡ä»¶å†…å®¹\n",
    "            with open(filePath, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "            \n",
    "            lines = content.split('\\n')\n",
    "            result['debug_info']['total_lines'] = len(lines)\n",
    "            \n",
    "            # æ‰¾åˆ°ç›®æ ‡æ ‡é¢˜åœ¨æ ‡é¢˜åˆ—è¡¨ä¸­çš„ä½ç½®\n",
    "            targetIndex = self._findHeadingIndex(headingList, targetHeading)\n",
    "            result['debug_info']['target_index'] = targetIndex\n",
    "            \n",
    "            if targetIndex == -1:\n",
    "                result['debug_info']['error'] = f\"åœ¨æ ‡é¢˜åˆ—è¡¨ä¸­æœªæ‰¾åˆ°ç›®æ ‡æ ‡é¢˜ '{targetHeading}'\"\n",
    "                return result\n",
    "            \n",
    "            # æ‰¾åˆ°ç›®æ ‡æ ‡é¢˜åœ¨æ–‡ä»¶ä¸­çš„ä½ç½®\n",
    "            targetLineIndex = self._findHeadingInFile(lines, targetHeading)\n",
    "            result['debug_info']['start_line'] = targetLineIndex\n",
    "            \n",
    "            if targetLineIndex == -1:\n",
    "                result['debug_info']['error'] = f\"åœ¨æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ç›®æ ‡æ ‡é¢˜ '{targetHeading}'\"\n",
    "                return result\n",
    "            \n",
    "            # ç¡®å®šç»“æŸä½ç½®\n",
    "            if targetIndex == len(headingList) - 1:\n",
    "                # æœ€åä¸€ä¸ªæ ‡é¢˜\n",
    "                endLineIndex = len(lines)\n",
    "                result['debug_info']['next_heading'] = 'æ–‡ä»¶ç»“æŸ'\n",
    "            else:\n",
    "                # æ‰¾åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜\n",
    "                nextHeading = headingList[targetIndex + 1]\n",
    "                result['debug_info']['next_heading'] = nextHeading\n",
    "                endLineIndex = self._findHeadingInFile(lines, nextHeading)\n",
    "                \n",
    "                if endLineIndex == -1:\n",
    "                    result['debug_info']['error'] = f\"åœ¨æ–‡ä»¶ä¸­æœªæ‰¾åˆ°ä¸‹ä¸€ä¸ªæ ‡é¢˜ '{nextHeading}'\"\n",
    "                    return result\n",
    "            \n",
    "            result['debug_info']['end_line'] = endLineIndex\n",
    "            \n",
    "            # æå–å†…å®¹\n",
    "            extractedLines = lines[targetLineIndex:endLineIndex]\n",
    "            extractedContent = '\\n'.join(extractedLines)\n",
    "            \n",
    "            result['success'] = True\n",
    "            result['content'] = extractedContent.strip()\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            result['debug_info']['error'] = f\"æ–‡ä»¶ '{filePath}' ä¸å­˜åœ¨\"\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            result['debug_info']['error'] = f\"å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿå¼‚å¸¸ - {str(e)}\"\n",
    "            return result\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•åŠŸèƒ½\"\"\"\n",
    "#     extractor = ContentExtractor()\n",
    "    \n",
    "#     # æµ‹è¯•æ•°æ®\n",
    "#     headingList = [\n",
    "#         '# A Q-learning approach to the continuous control problem of robot inverted pendulum balancing',\n",
    "#         '# Corresponding Author:',\n",
    "#         '# A Q-learning approach to the continuous control problem of robot inverted pendulum balancing',\n",
    "#         '# Abstract',\n",
    "#         '# 1. Introduction',\n",
    "#         '# 2. Proposed approach and background',\n",
    "#         '# 3. Methodologies',\n",
    "#         '# 4. Results and discussion',\n",
    "#         '# 5. Conclusion',\n",
    "#         '# Acknowledgements',\n",
    "#         '# References'\n",
    "#     ]\n",
    "    \n",
    "#     filePath = \"/Users/xiaokong/task/2025/paper_vis/vis/md/2dbbabd2678ba74fcd9b08aadae975ae.md\"\n",
    "#     targetHeading = \"# 5. Conclusion\"\n",
    "    \n",
    "#     print(\"=== æµ‹è¯•å†…å®¹æå–åŠŸèƒ½ ===\")\n",
    "#     print(f\"ç›®æ ‡æ ‡é¢˜: {targetHeading}\")\n",
    "#     print(f\"æ–‡ä»¶è·¯å¾„: {filePath}\")\n",
    "#     print()\n",
    "    \n",
    "#     # æµ‹è¯•åŸºæœ¬åŠŸèƒ½\n",
    "#     content = extractor.extractContentByHeading(headingList, filePath, targetHeading)\n",
    "#     print(\"=== æå–çš„å†…å®¹ ===\")\n",
    "#     print(content)\n",
    "#     print()\n",
    "    \n",
    "#     # æµ‹è¯•è°ƒè¯•åŠŸèƒ½\n",
    "#     debugResult = extractor.extractContentByHeadingWithDebug(headingList, filePath, targetHeading)\n",
    "#     print(\"=== è°ƒè¯•ä¿¡æ¯ ===\")\n",
    "#     print(f\"æˆåŠŸ: {debugResult['success']}\")\n",
    "#     print(f\"ç›®æ ‡æ ‡é¢˜: {debugResult['debug_info']['target_heading']}\")\n",
    "#     print(f\"ç›®æ ‡ç´¢å¼•: {debugResult['debug_info']['target_index']}\")\n",
    "#     print(f\"ä¸‹ä¸€ä¸ªæ ‡é¢˜: {debugResult['debug_info']['next_heading']}\")\n",
    "#     print(f\"å¼€å§‹è¡Œ: {debugResult['debug_info']['start_line']}\")\n",
    "#     print(f\"ç»“æŸè¡Œ: {debugResult['debug_info']['end_line']}\")\n",
    "#     print(f\"æ€»è¡Œæ•°: {debugResult['debug_info']['total_lines']}\")\n",
    "#     if debugResult['debug_info']['error']:\n",
    "#         print(f\"é”™è¯¯: {debugResult['debug_info']['error']}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd04e90",
   "metadata": {},
   "source": [
    "# ä»markdownä¸­æ‰¾åˆ°æ‰€æœ‰æ¨¡å—çš„å†…å®¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dbc09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "ç»¼åˆå†…å®¹æå–å™¨\n",
    "æ•´åˆæ ‡é¢˜è§„èŒƒåŒ–ã€æ ‡é¢˜æ˜ å°„å’Œå†…å®¹æå–åŠŸèƒ½\n",
    "\n",
    "åŠŸèƒ½æµç¨‹ï¼š\n",
    "1. ä½¿ç”¨NormalizeHeadings.pyè·å–æ¸…æ´—åçš„ä¸€çº§æ ‡é¢˜åˆ—è¡¨\n",
    "2. ä½¿ç”¨TitleMappingLLM.pyå°†æ ‡é¢˜æ˜ å°„åˆ°å››ä¸ªæ ‡å‡†æ³³é“\n",
    "3. ä½¿ç”¨extractContentByHeading.pyæ ¹æ®æ˜ å°„ç»“æœæå–å…·ä½“å†…å®¹\n",
    "4. è¿”å›æŒ‰æ³³é“ç»„ç»‡çš„å®Œæ•´å†…å®¹å­—å…¸\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from typing import Dict, List\n",
    "from NormalizeHeadings import HeadingNormalizer\n",
    "from TitleMappingLLM import TitleMappingLLM\n",
    "from extractContentByHeading import ContentExtractor\n",
    "\n",
    "\n",
    "class ComprehensiveContentExtractor:\n",
    "    \"\"\"ç»¼åˆå†…å®¹æå–å™¨\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"åˆå§‹åŒ–ç»¼åˆæå–å™¨\"\"\"\n",
    "        self.heading_normalizer = HeadingNormalizer()\n",
    "        self.title_mapper = TitleMappingLLM()\n",
    "        self.content_extractor = ContentExtractor()\n",
    "    \n",
    "    def extract_comprehensive_content(self, markdown_file_path: str) -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        ç»¼åˆå†…å®¹æå–ä¸»å‡½æ•°\n",
    "        \n",
    "        Args:\n",
    "            markdown_file_path: Markdownæ–‡ä»¶è·¯å¾„\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, str]: æŒ‰å››ä¸ªæ ‡å‡†æ³³é“ç»„ç»‡çš„å®Œæ•´å†…å®¹å­—å…¸\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # æ­¥éª¤1: è·å–æ¸…æ´—åçš„ä¸€çº§æ ‡é¢˜åˆ—è¡¨\n",
    "            print(\"æ­¥éª¤1: è·å–æ¸…æ´—åçš„ä¸€çº§æ ‡é¢˜åˆ—è¡¨...\")\n",
    "            h1_headings = self.heading_normalizer.process_markdown_file(markdown_file_path)\n",
    "            \n",
    "            if not h1_headings:\n",
    "                print(\"é”™è¯¯: æœªèƒ½æå–åˆ°ä¸€çº§æ ‡é¢˜\")\n",
    "                return {}\n",
    "            \n",
    "            print(f\"æå–åˆ° {len(h1_headings)} ä¸ªä¸€çº§æ ‡é¢˜:\")\n",
    "            for i, heading in enumerate(h1_headings, 1):\n",
    "                print(f\"  {i}. {heading}\")\n",
    "            print()\n",
    "            \n",
    "            # æ­¥éª¤2: å°†æ ‡é¢˜æ˜ å°„åˆ°å››ä¸ªæ ‡å‡†æ³³é“\n",
    "            print(\"æ­¥éª¤2: å°†æ ‡é¢˜æ˜ å°„åˆ°å››ä¸ªæ ‡å‡†æ³³é“...\")\n",
    "            mapping_result = self.title_mapper.map_titles(h1_headings)\n",
    "            \n",
    "            if not mapping_result:\n",
    "                print(\"é”™è¯¯: æ ‡é¢˜æ˜ å°„å¤±è´¥\")\n",
    "                return {}\n",
    "            \n",
    "            print(\"æ˜ å°„ç»“æœ:\")\n",
    "            for lane, titles in mapping_result.items():\n",
    "                print(f\"  {lane}: {titles}\")\n",
    "            print()\n",
    "            \n",
    "            # æ­¥éª¤3: æ ¹æ®æ˜ å°„ç»“æœæå–å…·ä½“å†…å®¹\n",
    "            print(\"æ­¥éª¤3: æå–å„æ³³é“çš„å…·ä½“å†…å®¹...\")\n",
    "            final_result = {}\n",
    "            \n",
    "            for lane_name, mapped_titles in mapping_result.items():\n",
    "                print(f\"å¤„ç†æ³³é“: {lane_name}\")\n",
    "                lane_content = \"\"\n",
    "                \n",
    "                for title in mapped_titles:\n",
    "                    print(f\"  æå–æ ‡é¢˜: {title}\")\n",
    "                    content = self.content_extractor.extractContentByHeading(\n",
    "                        h1_headings, markdown_file_path, title\n",
    "                    )\n",
    "                    \n",
    "                    # æ£€æŸ¥æ˜¯å¦æå–æˆåŠŸ\n",
    "                    if content.startswith(\"é”™è¯¯ï¼š\"):\n",
    "                        print(f\"    è­¦å‘Š: {content}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # æ‹¼æ¥å†…å®¹\n",
    "                    if lane_content:\n",
    "                        lane_content += \"\\n\\n\" + content\n",
    "                    else:\n",
    "                        lane_content = content\n",
    "                    \n",
    "                    print(f\"    æˆåŠŸæå– {len(content)} ä¸ªå­—ç¬¦\")\n",
    "                \n",
    "                final_result[lane_name] = lane_content\n",
    "                print(f\"  æ³³é“ '{lane_name}' æ€»å†…å®¹é•¿åº¦: {len(lane_content)} å­—ç¬¦\")\n",
    "                print()\n",
    "            \n",
    "            print(\"=== ç»¼åˆå†…å®¹æå–å®Œæˆ ===\")\n",
    "            return final_result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ç»¼åˆå†…å®¹æå–è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def extract_content_with_summary(self, markdown_file_path: str) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        å¸¦æ‘˜è¦ä¿¡æ¯çš„ç»¼åˆå†…å®¹æå–\n",
    "        \n",
    "        Args:\n",
    "            markdown_file_path: Markdownæ–‡ä»¶è·¯å¾„\n",
    "        \n",
    "        Returns:\n",
    "            Dict[str, any]: åŒ…å«å†…å®¹å’Œæ‘˜è¦ä¿¡æ¯çš„å­—å…¸\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # è·å–æ¸…æ´—åçš„ä¸€çº§æ ‡é¢˜åˆ—è¡¨\n",
    "            h1_headings = self.heading_normalizer.process_markdown_file(markdown_file_path)\n",
    "            \n",
    "            if not h1_headings:\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'error': 'æœªèƒ½æå–åˆ°ä¸€çº§æ ‡é¢˜',\n",
    "                    'content': {}\n",
    "                }\n",
    "            \n",
    "            # å°†æ ‡é¢˜æ˜ å°„åˆ°å››ä¸ªæ ‡å‡†æ³³é“\n",
    "            mapping_result = self.title_mapper.map_titles(h1_headings)\n",
    "            \n",
    "            if not mapping_result:\n",
    "                return {\n",
    "                    'success': False,\n",
    "                    'error': 'æ ‡é¢˜æ˜ å°„å¤±è´¥',\n",
    "                    'content': {}\n",
    "                }\n",
    "            \n",
    "            # æå–å…·ä½“å†…å®¹\n",
    "            final_result = {}\n",
    "            extraction_stats = {}\n",
    "            \n",
    "            for lane_name, mapped_titles in mapping_result.items():\n",
    "                lane_content = \"\"\n",
    "                extraction_stats[lane_name] = {\n",
    "                    'titles_count': len(mapped_titles),\n",
    "                    'titles': mapped_titles,\n",
    "                    'successful_extractions': 0,\n",
    "                    'failed_extractions': 0\n",
    "                }\n",
    "                \n",
    "                for title in mapped_titles:\n",
    "                    content = self.content_extractor.extractContentByHeading(\n",
    "                        h1_headings, markdown_file_path, title\n",
    "                    )\n",
    "                    \n",
    "                    if content.startswith(\"é”™è¯¯ï¼š\"):\n",
    "                        extraction_stats[lane_name]['failed_extractions'] += 1\n",
    "                        continue\n",
    "                    \n",
    "                    extraction_stats[lane_name]['successful_extractions'] += 1\n",
    "                    \n",
    "                    if lane_content:\n",
    "                        lane_content += \"\\n\\n\" + content\n",
    "                    else:\n",
    "                        lane_content = content\n",
    "                \n",
    "                final_result[lane_name] = lane_content\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'content': final_result,\n",
    "                'summary': {\n",
    "                    'total_h1_headings': len(h1_headings),\n",
    "                    'h1_headings': h1_headings,\n",
    "                    'mapping_result': mapping_result,\n",
    "                    'extraction_stats': extraction_stats,\n",
    "                    'file_path': markdown_file_path\n",
    "                }\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'error': f'å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}',\n",
    "                'content': {}\n",
    "            }\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•ç»¼åˆå†…å®¹æå–åŠŸèƒ½\"\"\"\n",
    "    print(\"=== ç»¼åˆå†…å®¹æå–å™¨æµ‹è¯• ===\")\n",
    "    \n",
    "    # åˆ›å»ºç»¼åˆæå–å™¨å®ä¾‹\n",
    "    extractor = ComprehensiveContentExtractor()\n",
    "    \n",
    "    # æµ‹è¯•æ–‡ä»¶è·¯å¾„ï¼ˆè¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ï¼‰\n",
    "    test_file_path = \"/Users/xiaokong/task/2025/paper_vis/vis/md/2dbbabd2678ba74fcd9b08aadae975ae.md\"\n",
    "    \n",
    "    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨\n",
    "    if not os.path.exists(test_file_path):\n",
    "        print(f\"é”™è¯¯: æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file_path}\")\n",
    "        print(\"è¯·ä¿®æ”¹ test_file_path å˜é‡ä¸ºå®é™…å­˜åœ¨çš„Markdownæ–‡ä»¶è·¯å¾„\")\n",
    "        return\n",
    "    \n",
    "    print(f\"æµ‹è¯•æ–‡ä»¶: {test_file_path}\")\n",
    "    print()\n",
    "    \n",
    "    # æ‰§è¡Œç»¼åˆå†…å®¹æå–\n",
    "    result = extractor.extract_comprehensive_content(test_file_path)\n",
    "    \n",
    "    if result:\n",
    "        print(\"=== æœ€ç»ˆç»“æœ ===\")\n",
    "        for lane_name, content in result.items():\n",
    "            print(f\"\\næ³³é“: {lane_name}\")\n",
    "            print(f\"å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦\")\n",
    "            print(f\"å†…å®¹é¢„è§ˆ: {content}...\")\n",
    "            print(\"-\" * 50)\n",
    "    else:\n",
    "        print(\"å†…å®¹æå–å¤±è´¥\")\n",
    "    \n",
    "    print(\"\\n=== æµ‹è¯•å®Œæˆ ===\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de927faf",
   "metadata": {},
   "source": [
    "# åŒ¹é…å›¾è¡¨ä½ç½®"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aaadde",
   "metadata": {},
   "source": [
    "{\n",
    "  \"Context & Related Work\": [\n",
    "    {\n",
    "      \"figure_id\": \"b357bf6b985700391e95e23414e4b79c293d3fc25bdec3f58908049d50871fa7\", \n",
    "      \"figure_caption\": \"....\",\n",
    "      \"reference_text\": [\"....\",\"...\"ï¼Œâ€...â€œ] \n",
    "    }\n",
    "  ],\n",
    "  \"Methodology & Setup\": [\n",
    "    {\n",
    "      \"figure_id\": \"80b3149cfde69122d789174087de284b1b7ca2a42efb05472127cf3a3416e08e\",\n",
    "      \"figure_caption\": \"....\",\n",
    "      \"reference_text\": [\"....\"]\n",
    "    },\n",
    "    // ... æ›´å¤šå›¾è¡¨\n",
    "  ],\n",
    "  \"Results & Analysis\": [],\n",
    "  \"Conclusion\": []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f2127d",
   "metadata": {},
   "source": [
    "1. é€šè¿‡ComprehensiveContentExtractorè·å–åˆ°æœ€ç»ˆéœ€è¦æŠ½å–ç‚¹çš„åŸå§‹æ–‡æœ¬\n",
    "2. é€šè¿‡merge_dataå¾—åˆ°mergedataï¼Œæ‰§è¡Œget_figureå¾—åˆ°å›¾è¡¨æ•°æ®\n",
    "3. éå†æ¯ä¸ªcaptionå»åŒ¹é…åˆ°æ³³é“\n",
    "4. å½¢æˆfigure_mapæ•°æ®ä»¥ä½œä¸ºå›¾è¡¨å¯è§†åŒ–æ•°æ®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e188082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sumy\n",
      "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: docopt<0.7,>=0.6.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from sumy) (0.6.2)\n",
      "Collecting breadability>=0.1.20 (from sumy)\n",
      "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.7.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from sumy) (2.32.5)\n",
      "Collecting pycountry>=18.2.23 (from sumy)\n",
      "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: nltk>=3.0.2 in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from sumy) (3.9.1)\n",
      "Requirement already satisfied: chardet in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
      "Requirement already satisfied: lxml>=2.0 in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from breadability>=0.1.20->sumy) (5.3.1)\n",
      "Requirement already satisfied: click in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from nltk>=3.0.2->sumy) (8.1.8)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from nltk>=3.0.2->sumy) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from requests>=2.7.0->sumy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from requests>=2.7.0->sumy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from requests>=2.7.0->sumy) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/Caskroom/miniconda/base/envs/ser/lib/python3.12/site-packages (from requests>=2.7.0->sumy) (2025.1.31)\n",
      "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
      "Downloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: breadability\n",
      "\u001b[33m  DEPRECATION: Building 'breadability' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'breadability'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for breadability (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21795 sha256=44e31405e7c6bf59cc3539360dc277dff0ffb75e3c3470aed4e3dea5a268db9f\n",
      "  Stored in directory: /Users/xiaokong/Library/Caches/pip/wheels/32/99/64/59305409cacd03aa03e7bddf31a9db34b1fa7033bd41972662\n",
      "Successfully built breadability\n",
      "Installing collected packages: pycountry, breadability, sumy\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/3\u001b[0m [sumy]\n",
      "\u001b[1A\u001b[2KSuccessfully installed breadability-0.1.20 pycountry-24.6.1 sumy-0.11.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sumy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d3e8e",
   "metadata": {},
   "source": [
    "# æ€»è°ƒåº¦å™¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230e3a36",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea142fff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96382db0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
